{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP) & Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "#dir(nltk.corpus)\n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\inaugural.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\webtext.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\nps_chat.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('nps_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\genesis.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('genesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so above, i have downloaded the resources individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'inaugural', 'inaugural.zip', 'nps_chat', 'nps_chat.zip', 'treebank', 'treebank.zip', 'webtext', 'webtext.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's have a look on feilds of gutenberg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so we have all the text files\n",
    "### let's take one text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words(\"shakespeare-hamlet.txt\")\n",
    "hamlet\n",
    "#displays elements of hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "#look at first 500 elements\n",
    "for word in hamlet[:500]:\n",
    "    print(word, sep=' ', end=' ')\n",
    "#we can use lot of these files to analysis and texts\n",
    "#for understanding and analysis purposes>this is where analytics comes in and \n",
    "#allow us to leran diff features and diff applications in langauge processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below i have created a paragraph regarding AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = \"\"\"Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science AI research is defined as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\"\n",
    "My Engineering and Master’s in Mathematics, Electronics, Telecommunications and Computer Science have given me good knowledge in Algorithms, Telecom, Databases, Programming, Web, Networks and Software Security. I had a great opportunity to worked as a Junior Data Scientist and Full Stack Developer during my Stage for 6 months. I was involved in 2 Data Science projects (Recommendation Application System and Sales Forecast) and 4 of software development projects. Then after finishing my internship, I have taken Data Science, Machine Learning, Deep Learning and Neural Network courses to dig into more with my personal interest and passion. I have done few freelancing and self-practice projects in Data Science which made me confidant enough to apply for this job. \n",
    "\n",
    "The Data Science projects which I worked during my internship, personal, academic and freelancing projects which motivates me to move into this field. I have good knowledge in different Data Science domains where there to discover things, active participation during the process-build-Test and takes responsibilities until I met the requirements in order to give the perfect output to the right time. I have great enthusiasm and passion towards this field which makes me enough confident to learn more dig into deeper.\n",
    "\n",
    "The projects in which I worked, gave me very good knowledge in Programming, developing Algorithms, Database, Data Science, Machine Learning, Statistics, Predicting Models, Deep Learning AI. I also have good knowledge in Python, scikit-learn, NLP, Pandas, C, SQL, Java, Spark, TensorFlow, Keras, Big data, MLlib etc. I would like to be part of your team through this offer to participate in great work/research of making things much easier in a good way by providing better and great services to the world.  \n",
    ".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as we got string type, so it's makes us easy to tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize will help us to tokenize all our words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " ',',\n",
       " 'sometimes',\n",
       " 'called',\n",
       " 'machine',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'is',\n",
       " 'intelligence',\n",
       " 'demonstrated',\n",
       " 'by',\n",
       " 'machines',\n",
       " ',',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'displayed',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'other',\n",
       " 'animals',\n",
       " '.',\n",
       " 'In',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'AI',\n",
       " 'research',\n",
       " 'is',\n",
       " 'defined',\n",
       " 'as',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " '``',\n",
       " 'intelligent',\n",
       " 'agents',\n",
       " \"''\",\n",
       " ':',\n",
       " 'any',\n",
       " 'device',\n",
       " 'that',\n",
       " 'perceives',\n",
       " 'its',\n",
       " 'environment',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'actions',\n",
       " 'that',\n",
       " 'maximize',\n",
       " 'its',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'successfully',\n",
       " 'achieving',\n",
       " 'its',\n",
       " 'goals',\n",
       " '.',\n",
       " 'Colloquially',\n",
       " ',',\n",
       " 'the',\n",
       " 'term',\n",
       " '``',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'applied',\n",
       " 'when',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'mimics',\n",
       " '``',\n",
       " 'cognitive',\n",
       " \"''\",\n",
       " 'functions',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'associate',\n",
       " 'with',\n",
       " 'other',\n",
       " 'human',\n",
       " 'minds',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " '``',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'problem',\n",
       " 'solving',\n",
       " \"''\",\n",
       " 'My',\n",
       " 'Engineering',\n",
       " 'and',\n",
       " 'Master',\n",
       " '’',\n",
       " 's',\n",
       " 'in',\n",
       " 'Mathematics',\n",
       " ',',\n",
       " 'Electronics',\n",
       " ',',\n",
       " 'Telecommunications',\n",
       " 'and',\n",
       " 'Computer',\n",
       " 'Science',\n",
       " 'have',\n",
       " 'given',\n",
       " 'me',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'Algorithms',\n",
       " ',',\n",
       " 'Telecom',\n",
       " ',',\n",
       " 'Databases',\n",
       " ',',\n",
       " 'Programming',\n",
       " ',',\n",
       " 'Web',\n",
       " ',',\n",
       " 'Networks',\n",
       " 'and',\n",
       " 'Software',\n",
       " 'Security',\n",
       " '.',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " 'great',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'worked',\n",
       " 'as',\n",
       " 'a',\n",
       " 'Junior',\n",
       " 'Data',\n",
       " 'Scientist',\n",
       " 'and',\n",
       " 'Full',\n",
       " 'Stack',\n",
       " 'Developer',\n",
       " 'during',\n",
       " 'my',\n",
       " 'Stage',\n",
       " 'for',\n",
       " '6',\n",
       " 'months',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'involved',\n",
       " 'in',\n",
       " '2',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'projects',\n",
       " '(',\n",
       " 'Recommendation',\n",
       " 'Application',\n",
       " 'System',\n",
       " 'and',\n",
       " 'Sales',\n",
       " 'Forecast',\n",
       " ')',\n",
       " 'and',\n",
       " '4',\n",
       " 'of',\n",
       " 'software',\n",
       " 'development',\n",
       " 'projects',\n",
       " '.',\n",
       " 'Then',\n",
       " 'after',\n",
       " 'finishing',\n",
       " 'my',\n",
       " 'internship',\n",
       " ',',\n",
       " 'I',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'Data',\n",
       " 'Science',\n",
       " ',',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " ',',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'and',\n",
       " 'Neural',\n",
       " 'Network',\n",
       " 'courses',\n",
       " 'to',\n",
       " 'dig',\n",
       " 'into',\n",
       " 'more',\n",
       " 'with',\n",
       " 'my',\n",
       " 'personal',\n",
       " 'interest',\n",
       " 'and',\n",
       " 'passion',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'done',\n",
       " 'few',\n",
       " 'freelancing',\n",
       " 'and',\n",
       " 'self-practice',\n",
       " 'projects',\n",
       " 'in',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'which',\n",
       " 'made',\n",
       " 'me',\n",
       " 'confidant',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'apply',\n",
       " 'for',\n",
       " 'this',\n",
       " 'job',\n",
       " '.',\n",
       " 'The',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'projects',\n",
       " 'which',\n",
       " 'I',\n",
       " 'worked',\n",
       " 'during',\n",
       " 'my',\n",
       " 'internship',\n",
       " ',',\n",
       " 'personal',\n",
       " ',',\n",
       " 'academic',\n",
       " 'and',\n",
       " 'freelancing',\n",
       " 'projects',\n",
       " 'which',\n",
       " 'motivates',\n",
       " 'me',\n",
       " 'to',\n",
       " 'move',\n",
       " 'into',\n",
       " 'this',\n",
       " 'field',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'different',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'domains',\n",
       " 'where',\n",
       " 'there',\n",
       " 'to',\n",
       " 'discover',\n",
       " 'things',\n",
       " ',',\n",
       " 'active',\n",
       " 'participation',\n",
       " 'during',\n",
       " 'the',\n",
       " 'process-build-Test',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'responsibilities',\n",
       " 'until',\n",
       " 'I',\n",
       " 'met',\n",
       " 'the',\n",
       " 'requirements',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'give',\n",
       " 'the',\n",
       " 'perfect',\n",
       " 'output',\n",
       " 'to',\n",
       " 'the',\n",
       " 'right',\n",
       " 'time',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'great',\n",
       " 'enthusiasm',\n",
       " 'and',\n",
       " 'passion',\n",
       " 'towards',\n",
       " 'this',\n",
       " 'field',\n",
       " 'which',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'enough',\n",
       " 'confident',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'more',\n",
       " 'dig',\n",
       " 'into',\n",
       " 'deeper',\n",
       " '.',\n",
       " 'The',\n",
       " 'projects',\n",
       " 'in',\n",
       " 'which',\n",
       " 'I',\n",
       " 'worked',\n",
       " ',',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'very',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'Programming',\n",
       " ',',\n",
       " 'developing',\n",
       " 'Algorithms',\n",
       " ',',\n",
       " 'Database',\n",
       " ',',\n",
       " 'Data',\n",
       " 'Science',\n",
       " ',',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " ',',\n",
       " 'Statistics',\n",
       " ',',\n",
       " 'Predicting',\n",
       " 'Models',\n",
       " ',',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'AI',\n",
       " '.',\n",
       " 'I',\n",
       " 'also',\n",
       " 'have',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'Python',\n",
       " ',',\n",
       " 'scikit-learn',\n",
       " ',',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'Pandas',\n",
       " ',',\n",
       " 'C',\n",
       " ',',\n",
       " 'SQL',\n",
       " ',',\n",
       " 'Java',\n",
       " ',',\n",
       " 'Spark',\n",
       " ',',\n",
       " 'TensorFlow',\n",
       " ',',\n",
       " 'Keras',\n",
       " ',',\n",
       " 'Big',\n",
       " 'data',\n",
       " ',',\n",
       " 'MLlib',\n",
       " 'etc',\n",
       " '.',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'be',\n",
       " 'part',\n",
       " 'of',\n",
       " 'your',\n",
       " 'team',\n",
       " 'through',\n",
       " 'this',\n",
       " 'offer',\n",
       " 'to',\n",
       " 'participate',\n",
       " 'in',\n",
       " 'great',\n",
       " 'work/research',\n",
       " 'of',\n",
       " 'making',\n",
       " 'things',\n",
       " 'much',\n",
       " 'easier',\n",
       " 'in',\n",
       " 'a',\n",
       " 'good',\n",
       " 'way',\n",
       " 'by',\n",
       " 'providing',\n",
       " 'better',\n",
       " 'and',\n",
       " 'great',\n",
       " 'services',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_tokens = word_tokenize(AI)\n",
    "AI_tokens\n",
    "#it displays all the inputs are provided into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thee 102 tokens are list of workd and special charecteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find out the word count of all the words in paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 37, 'and': 16, '.': 14, 'in': 13, 'to': 12, 'i': 11, 'the': 10, 'science': 8, 'data': 8, 'have': 6, ...})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 37),\n",
       " ('and', 16),\n",
       " ('.', 14),\n",
       " ('in', 13),\n",
       " ('to', 12),\n",
       " ('i', 11),\n",
       " ('the', 10),\n",
       " ('science', 8),\n",
       " ('data', 8),\n",
       " ('have', 6)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's have a look on top 10 tokens with high freequency \n",
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#blankline tokenize\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank =blankline_tokenize(AI)\n",
    "len(AI_blank)\n",
    "#it says howmany paragraphs we have and what all paragraphs are seperated by a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams > token of 2 consecutive written words\n",
    "#Trigrams > token of 3 consecutive written words\n",
    "#Ngrams > toekn of n consecutive written words\n",
    "from nltk.util import ngrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'an',\n",
       " 'open-source',\n",
       " ',',\n",
       " 'distributed',\n",
       " 'processing',\n",
       " 'system',\n",
       " 'used',\n",
       " 'for',\n",
       " 'big',\n",
       " 'data',\n",
       " 'workloads',\n",
       " '.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's creat our own string\n",
    "string = \"Apache Spark is an open-source, distributed processing system used for big data workloads.\"\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 'Spark'),\n",
       " ('Spark', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'open-source'),\n",
       " ('open-source', ','),\n",
       " (',', 'distributed'),\n",
       " ('distributed', 'processing'),\n",
       " ('processing', 'system'),\n",
       " ('system', 'used'),\n",
       " ('used', 'for'),\n",
       " ('for', 'big'),\n",
       " ('big', 'data'),\n",
       " ('data', 'workloads'),\n",
       " ('workloads', '.')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we pass all these tokens (in the form of lists)into ngrams, bigrams, trigrams\n",
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams\n",
    "#displays tokens in a pair form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 'Spark', 'is'),\n",
       " ('Spark', 'is', 'an'),\n",
       " ('is', 'an', 'open-source'),\n",
       " ('an', 'open-source', ','),\n",
       " ('open-source', ',', 'distributed'),\n",
       " (',', 'distributed', 'processing'),\n",
       " ('distributed', 'processing', 'system'),\n",
       " ('processing', 'system', 'used'),\n",
       " ('system', 'used', 'for'),\n",
       " ('used', 'for', 'big'),\n",
       " ('for', 'big', 'data'),\n",
       " ('big', 'data', 'workloads'),\n",
       " ('data', 'workloads', '.')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see tokens in the form of 3 words\n",
    "quotes_trigram = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 'Spark', 'is', 'an', 'open-source'),\n",
       " ('Spark', 'is', 'an', 'open-source', ','),\n",
       " ('is', 'an', 'open-source', ',', 'distributed'),\n",
       " ('an', 'open-source', ',', 'distributed', 'processing'),\n",
       " ('open-source', ',', 'distributed', 'processing', 'system'),\n",
       " (',', 'distributed', 'processing', 'system', 'used'),\n",
       " ('distributed', 'processing', 'system', 'used', 'for'),\n",
       " ('processing', 'system', 'used', 'for', 'big'),\n",
       " ('system', 'used', 'for', 'big', 'data'),\n",
       " ('used', 'for', 'big', 'data', 'workloads'),\n",
       " ('for', 'big', 'data', 'workloads', '.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ngrams here we need to change N value if you say N=5 (gives form of 5 words)\n",
    "quotes_ngram = list(nltk.ngrams(quotes_tokens, 5))\n",
    "quotes_ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Tokenization, we have to do Stemming\n",
    "### Stemming: Normalize words into it’s base form or root form (considers prefixes and suffixes of word and gives/works with root word)\n",
    "### Ex: root word: Affect = affecting, affects, affected, affection, affecting etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst =PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'process'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "given:given\n",
      "giving:give\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = [\"give\", \"given\", \"giving\", \"gave\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "given:giv\n",
      "giving:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "#let's try with another stemmer : LancasterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +lst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seems to be LancasterStemmer is more aggresive than PorterStemmer\n",
    "### stemmer depends on type of task that which you wanna perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization: groups together different inflected forms of words called lemma\n",
    "### >>Somehow, it similar to stemming as it maps several words into one common root\n",
    "### >>output of lemmatization is a proper word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer requires detailed dectionary (wordnet) as it gives output a proper root wrod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize(\"corpora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "given:given\n",
      "giving:giving\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+ \":\" +word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we got above out put because we didn't assign POS tags\n",
    "### POS tags tells us what the given word exactly wegher it's noun, verb, pronoun etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words: above, below, begin, really, all, from etc;\n",
    "### >> they are useful to make a sentence when you are speaking, but not useful in NLP when you are processing a language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's have a look on stopwords in english\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These stopwords are very important for formating a sentence, but not at all useful when you processing a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's have a look on stopwords in French\n",
    "stopwords.words(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 37),\n",
       " ('and', 16),\n",
       " ('.', 14),\n",
       " ('in', 13),\n",
       " ('to', 12),\n",
       " ('i', 11),\n",
       " ('the', 10),\n",
       " ('science', 8),\n",
       " ('data', 8),\n",
       " ('have', 6)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### except sceince and data there are no useful tokens above, so we have to remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "punctuations = re.compile(r'[:;-?!.,()0-9|]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuations = []\n",
    "for words in AI_tokens:\n",
    "    word=punctuations.sub(\"\",words)\n",
    "    if len(word)>0:\n",
    "        post_punctuations.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'AI',\n",
       " 'sometimes',\n",
       " 'called',\n",
       " 'machine',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'intelligence',\n",
       " 'demonstrated',\n",
       " 'by',\n",
       " 'machines',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'displayed',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'other',\n",
       " 'animals',\n",
       " 'In',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'AI',\n",
       " 'research',\n",
       " 'is',\n",
       " 'defined',\n",
       " 'as',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " '``',\n",
       " 'intelligent',\n",
       " 'agents',\n",
       " \"''\",\n",
       " 'any',\n",
       " 'device',\n",
       " 'that',\n",
       " 'perceives',\n",
       " 'its',\n",
       " 'environment',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'actions',\n",
       " 'that',\n",
       " 'maximize',\n",
       " 'its',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'successfully',\n",
       " 'achieving',\n",
       " 'its',\n",
       " 'goals',\n",
       " 'Colloquially',\n",
       " 'the',\n",
       " 'term',\n",
       " '``',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'applied',\n",
       " 'when',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'mimics',\n",
       " '``',\n",
       " 'cognitive',\n",
       " \"''\",\n",
       " 'functions',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'associate',\n",
       " 'with',\n",
       " 'other',\n",
       " 'human',\n",
       " 'minds',\n",
       " 'such',\n",
       " 'as',\n",
       " '``',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'problem',\n",
       " 'solving',\n",
       " \"''\",\n",
       " 'My',\n",
       " 'Engineering',\n",
       " 'and',\n",
       " 'Master',\n",
       " '’',\n",
       " 's',\n",
       " 'in',\n",
       " 'Mathematics',\n",
       " 'Electronics',\n",
       " 'Telecommunications',\n",
       " 'and',\n",
       " 'Computer',\n",
       " 'Science',\n",
       " 'have',\n",
       " 'given',\n",
       " 'me',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'Algorithms',\n",
       " 'Telecom',\n",
       " 'Databases',\n",
       " 'Programming',\n",
       " 'Web',\n",
       " 'Networks',\n",
       " 'and',\n",
       " 'Software',\n",
       " 'Security',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " 'great',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'worked',\n",
       " 'as',\n",
       " 'a',\n",
       " 'Junior',\n",
       " 'Data',\n",
       " 'Scientist',\n",
       " 'and',\n",
       " 'Full',\n",
       " 'Stack',\n",
       " 'Developer',\n",
       " 'during',\n",
       " 'my',\n",
       " 'Stage',\n",
       " 'for',\n",
       " 'months',\n",
       " 'I',\n",
       " 'was',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'projects',\n",
       " 'Recommendation',\n",
       " 'Application',\n",
       " 'System',\n",
       " 'and',\n",
       " 'Sales',\n",
       " 'Forecast',\n",
       " 'and',\n",
       " 'of',\n",
       " 'software',\n",
       " 'development',\n",
       " 'projects',\n",
       " 'Then',\n",
       " 'after',\n",
       " 'finishing',\n",
       " 'my',\n",
       " 'internship',\n",
       " 'I',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'and',\n",
       " 'Neural',\n",
       " 'Network',\n",
       " 'courses',\n",
       " 'to',\n",
       " 'dig',\n",
       " 'into',\n",
       " 'more',\n",
       " 'with',\n",
       " 'my',\n",
       " 'personal',\n",
       " 'interest',\n",
       " 'and',\n",
       " 'passion',\n",
       " 'I',\n",
       " 'have',\n",
       " 'done',\n",
       " 'few',\n",
       " 'freelancing',\n",
       " 'and',\n",
       " 'self-practice',\n",
       " 'projects',\n",
       " 'in',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'which',\n",
       " 'made',\n",
       " 'me',\n",
       " 'confidant',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'apply',\n",
       " 'for',\n",
       " 'this',\n",
       " 'job',\n",
       " 'The',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'projects',\n",
       " 'which',\n",
       " 'I',\n",
       " 'worked',\n",
       " 'during',\n",
       " 'my',\n",
       " 'internship',\n",
       " 'personal',\n",
       " 'academic',\n",
       " 'and',\n",
       " 'freelancing',\n",
       " 'projects',\n",
       " 'which',\n",
       " 'motivates',\n",
       " 'me',\n",
       " 'to',\n",
       " 'move',\n",
       " 'into',\n",
       " 'this',\n",
       " 'field',\n",
       " 'I',\n",
       " 'have',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'different',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'domains',\n",
       " 'where',\n",
       " 'there',\n",
       " 'to',\n",
       " 'discover',\n",
       " 'things',\n",
       " 'active',\n",
       " 'participation',\n",
       " 'during',\n",
       " 'the',\n",
       " 'process-build-Test',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'responsibilities',\n",
       " 'until',\n",
       " 'I',\n",
       " 'met',\n",
       " 'the',\n",
       " 'requirements',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'give',\n",
       " 'the',\n",
       " 'perfect',\n",
       " 'output',\n",
       " 'to',\n",
       " 'the',\n",
       " 'right',\n",
       " 'time',\n",
       " 'I',\n",
       " 'have',\n",
       " 'great',\n",
       " 'enthusiasm',\n",
       " 'and',\n",
       " 'passion',\n",
       " 'towards',\n",
       " 'this',\n",
       " 'field',\n",
       " 'which',\n",
       " 'makes',\n",
       " 'me',\n",
       " 'enough',\n",
       " 'confident',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'more',\n",
       " 'dig',\n",
       " 'into',\n",
       " 'deeper',\n",
       " 'The',\n",
       " 'projects',\n",
       " 'in',\n",
       " 'which',\n",
       " 'I',\n",
       " 'worked',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'very',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'Programming',\n",
       " 'developing',\n",
       " 'Algorithms',\n",
       " 'Database',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Statistics',\n",
       " 'Predicting',\n",
       " 'Models',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'AI',\n",
       " 'I',\n",
       " 'also',\n",
       " 'have',\n",
       " 'good',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'Python',\n",
       " 'scikit-learn',\n",
       " 'NLP',\n",
       " 'Pandas',\n",
       " 'C',\n",
       " 'SQL',\n",
       " 'Java',\n",
       " 'Spark',\n",
       " 'TensorFlow',\n",
       " 'Keras',\n",
       " 'Big',\n",
       " 'data',\n",
       " 'MLlib',\n",
       " 'etc',\n",
       " 'I',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'be',\n",
       " 'part',\n",
       " 'of',\n",
       " 'your',\n",
       " 'team',\n",
       " 'through',\n",
       " 'this',\n",
       " 'offer',\n",
       " 'to',\n",
       " 'participate',\n",
       " 'in',\n",
       " 'great',\n",
       " 'work/research',\n",
       " 'of',\n",
       " 'making',\n",
       " 'things',\n",
       " 'much',\n",
       " 'easier',\n",
       " 'in',\n",
       " 'a',\n",
       " 'good',\n",
       " 'way',\n",
       " 'by',\n",
       " 'providing',\n",
       " 'better',\n",
       " 'and',\n",
       " 'great',\n",
       " 'services',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### it is very important to remove unneccessary words while processing a language to more meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next important thing in NLp & Text mining/ text analysis is POS (parts of speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS: Tags and descriptions (gives the sense of a word) easy to evaluate while processing for more meaning\n",
    "### We can use POS tags as statistical NLP tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play with POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tags', 'NNS')]\n",
      "[('which', 'WDT')]\n",
      "[('gives', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[('sense', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('a', 'DT')]\n",
      "[('word', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('makes', 'VBZ')]\n",
      "[('easy', 'JJ')]\n",
      "[('to', 'TO')]\n",
      "[('evaluate', 'NN')]\n",
      "[('while', 'IN')]\n",
      "[('processing', 'NN')]\n",
      "[('language', 'NN')]\n",
      "[('for', 'IN')]\n",
      "[('more', 'RBR')]\n",
      "[('meaning', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#POS tags\n",
    "#create a new sentence\n",
    "sent = \"Tags which gives the sense of a word and makes easy to evaluate while processing language for more meaning\"\n",
    "#tokenize the sentence\n",
    "sent_token = word_tokenize(sent)\n",
    "#pass tags\n",
    "for token in sent_token:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kondal', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('eating', 'VBG')]\n",
      "[('a', 'DT')]\n",
      "[('delicious', 'JJ')]\n",
      "[('cake', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#let's take another example\n",
    "sent2 = \"Kondal is eating a delicious cake\"\n",
    "sent2_token = word_tokenize(sent2)\n",
    "for token in sent2_token:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are named entity reorganization?\n",
    "### Detecting a location name, person name, company name, movie, quantity value, organisation name etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sent = \"The US president stays in the WhiteHouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tokens = word_tokenize(NE_sent)\n",
    "NE_tags = nltk.pos_tag(NE_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (GSP US/NNP)\n",
      "  president/NN\n",
      "  stays/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION WhiteHouse/NNP))\n"
     ]
    }
   ],
   "source": [
    "#NER : name entered recognization\n",
    "NE_NER = ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is only possible because of POS tag, without POS tag it would be hard to detect the named entities of the given token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax: principles, rules and process\n",
    "### Syntax tree: It’s a tree representation of syntactic structure of sentences or strings\n",
    "### This structure is used to generating symbols, tables, compiler etc\n",
    "### Sentence >> noun phrase or prépositionnel phrase\n",
    "### Chunking : picking up individual pieces of information and grouping them into bigger pieces(chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('cat', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('mouse', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('fater', 'RB'),\n",
       " ('having', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = \"The cat had mouse which fater having a fresh cheese\"\n",
    "new_token = word_tokenize(new)\n",
    "new_tag = nltk.pos_tag(new_token)\n",
    "new_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are creating a chunk\n",
    "grammer_np = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "#means we want only what we mention in {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammer_np)\n",
    "#we have created regular expression matching string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT cat/NN)\n",
      "  had/VBD\n",
      "  (NP mouse/NN)\n",
      "  which/WDT\n",
      "  fater/RB\n",
      "  having/VBG\n",
      "  (NP a/DT fresh/JJ cheese/NN))\n"
     ]
    }
   ],
   "source": [
    "chunk_result = chunk_parser.parse(new_tag)\n",
    "print(chunk_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's apply all of them in Machine Learning classifier on the movie reviews from the analytic corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'inaugural', 'inaugural.zip', 'nps_chat', 'nps_chat.zip', 'stopwords', 'stopwords.zip', 'treebank', 'treebank.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at different elements in corpora \n",
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Palugudi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have 2 reviews neg=negative and pos=positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_reviews.fileids('pos')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt', 'pos/cv003_11664.txt', 'pos/cv004_11636.txt', 'pos/cv005_29443.txt', 'pos/cv006_15448.txt', 'pos/cv007_4968.txt', 'pos/cv008_29435.txt', 'pos/cv009_29592.txt', 'pos/cv010_29198.txt', 'pos/cv011_12166.txt', 'pos/cv012_29576.txt', 'pos/cv013_10159.txt', 'pos/cv014_13924.txt', 'pos/cv015_29439.txt', 'pos/cv016_4659.txt', 'pos/cv017_22464.txt', 'pos/cv018_20137.txt', 'pos/cv019_14482.txt', 'pos/cv020_8825.txt', 'pos/cv021_15838.txt', 'pos/cv022_12864.txt', 'pos/cv023_12672.txt', 'pos/cv024_6778.txt', 'pos/cv025_3108.txt', 'pos/cv026_29325.txt', 'pos/cv027_25219.txt', 'pos/cv028_26746.txt', 'pos/cv029_18643.txt', 'pos/cv030_21593.txt', 'pos/cv031_18452.txt', 'pos/cv032_22550.txt', 'pos/cv033_24444.txt', 'pos/cv034_29647.txt', 'pos/cv035_3954.txt', 'pos/cv036_16831.txt', 'pos/cv037_18510.txt', 'pos/cv038_9749.txt', 'pos/cv039_6170.txt', 'pos/cv040_8276.txt', 'pos/cv041_21113.txt', 'pos/cv042_10982.txt', 'pos/cv043_15013.txt', 'pos/cv044_16969.txt', 'pos/cv045_23923.txt', 'pos/cv046_10188.txt', 'pos/cv047_1754.txt', 'pos/cv048_16828.txt', 'pos/cv049_20471.txt', 'pos/cv050_11175.txt', 'pos/cv051_10306.txt', 'pos/cv052_29378.txt', 'pos/cv053_21822.txt', 'pos/cv054_4230.txt', 'pos/cv055_8338.txt', 'pos/cv056_13133.txt', 'pos/cv057_7453.txt', 'pos/cv058_8025.txt', 'pos/cv059_28885.txt', 'pos/cv060_10844.txt', 'pos/cv061_8837.txt', 'pos/cv062_23115.txt', 'pos/cv063_28997.txt', 'pos/cv064_24576.txt', 'pos/cv065_15248.txt', 'pos/cv066_10821.txt', 'pos/cv067_19774.txt', 'pos/cv068_13400.txt', 'pos/cv069_10801.txt', 'pos/cv070_12289.txt', 'pos/cv071_12095.txt', 'pos/cv072_6169.txt', 'pos/cv073_21785.txt', 'pos/cv074_6875.txt', 'pos/cv075_6500.txt', 'pos/cv076_24945.txt', 'pos/cv077_22138.txt', 'pos/cv078_14730.txt', 'pos/cv079_11933.txt', 'pos/cv080_13465.txt', 'pos/cv081_16582.txt', 'pos/cv082_11080.txt', 'pos/cv083_24234.txt', 'pos/cv084_13566.txt', 'pos/cv085_1381.txt', 'pos/cv086_18371.txt', 'pos/cv087_1989.txt', 'pos/cv088_24113.txt', 'pos/cv089_11418.txt', 'pos/cv090_0042.txt', 'pos/cv091_7400.txt', 'pos/cv092_28017.txt', 'pos/cv093_13951.txt', 'pos/cv094_27889.txt', 'pos/cv095_28892.txt', 'pos/cv096_11474.txt', 'pos/cv097_24970.txt', 'pos/cv098_15435.txt', 'pos/cv099_10534.txt', 'pos/cv100_11528.txt', 'pos/cv101_10175.txt', 'pos/cv102_7846.txt', 'pos/cv103_11021.txt', 'pos/cv104_18134.txt', 'pos/cv105_17990.txt', 'pos/cv106_16807.txt', 'pos/cv107_24319.txt', 'pos/cv108_15571.txt', 'pos/cv109_21172.txt', 'pos/cv110_27788.txt', 'pos/cv111_11473.txt', 'pos/cv112_11193.txt', 'pos/cv113_23102.txt', 'pos/cv114_18398.txt', 'pos/cv115_25396.txt', 'pos/cv116_28942.txt', 'pos/cv117_24295.txt', 'pos/cv118_28980.txt', 'pos/cv119_9867.txt', 'pos/cv120_4111.txt', 'pos/cv121_17302.txt', 'pos/cv122_7392.txt', 'pos/cv123_11182.txt', 'pos/cv124_4122.txt', 'pos/cv125_9391.txt', 'pos/cv126_28971.txt', 'pos/cv127_14711.txt', 'pos/cv128_29627.txt', 'pos/cv129_16741.txt', 'pos/cv130_17083.txt', 'pos/cv131_10713.txt', 'pos/cv132_5618.txt', 'pos/cv133_16336.txt', 'pos/cv134_22246.txt', 'pos/cv135_11603.txt', 'pos/cv136_11505.txt', 'pos/cv137_15422.txt', 'pos/cv138_12721.txt', 'pos/cv139_12873.txt', 'pos/cv140_7479.txt', 'pos/cv141_15686.txt', 'pos/cv142_22516.txt', 'pos/cv143_19666.txt', 'pos/cv144_5007.txt', 'pos/cv145_11472.txt', 'pos/cv146_18458.txt', 'pos/cv147_21193.txt', 'pos/cv148_16345.txt', 'pos/cv149_15670.txt', 'pos/cv150_12916.txt', 'pos/cv151_15771.txt', 'pos/cv152_8736.txt', 'pos/cv153_10779.txt', 'pos/cv154_9328.txt', 'pos/cv155_7308.txt', 'pos/cv156_10481.txt', 'pos/cv157_29372.txt', 'pos/cv158_10390.txt', 'pos/cv159_29505.txt', 'pos/cv160_10362.txt', 'pos/cv161_11425.txt', 'pos/cv162_10424.txt', 'pos/cv163_10052.txt', 'pos/cv164_22447.txt', 'pos/cv165_22619.txt', 'pos/cv166_11052.txt', 'pos/cv167_16376.txt', 'pos/cv168_7050.txt', 'pos/cv169_23778.txt', 'pos/cv170_3006.txt', 'pos/cv171_13537.txt', 'pos/cv172_11131.txt', 'pos/cv173_4471.txt', 'pos/cv174_9659.txt', 'pos/cv175_6964.txt', 'pos/cv176_12857.txt', 'pos/cv177_10367.txt', 'pos/cv178_12972.txt', 'pos/cv179_9228.txt', 'pos/cv180_16113.txt', 'pos/cv181_14401.txt', 'pos/cv182_7281.txt', 'pos/cv183_18612.txt', 'pos/cv184_2673.txt', 'pos/cv185_28654.txt', 'pos/cv186_2269.txt', 'pos/cv187_12829.txt', 'pos/cv188_19226.txt', 'pos/cv189_22934.txt', 'pos/cv190_27052.txt', 'pos/cv191_29719.txt', 'pos/cv192_14395.txt', 'pos/cv193_5416.txt', 'pos/cv194_12079.txt', 'pos/cv195_14528.txt', 'pos/cv196_29027.txt', 'pos/cv197_29328.txt', 'pos/cv198_18180.txt', 'pos/cv199_9629.txt', 'pos/cv200_2915.txt', 'pos/cv201_6997.txt', 'pos/cv202_10654.txt', 'pos/cv203_17986.txt', 'pos/cv204_8451.txt', 'pos/cv205_9457.txt', 'pos/cv206_14293.txt', 'pos/cv207_29284.txt', 'pos/cv208_9020.txt', 'pos/cv209_29118.txt', 'pos/cv210_9312.txt', 'pos/cv211_9953.txt', 'pos/cv212_10027.txt', 'pos/cv213_18934.txt', 'pos/cv214_12294.txt', 'pos/cv215_22240.txt', 'pos/cv216_18738.txt', 'pos/cv217_28842.txt', 'pos/cv218_24352.txt', 'pos/cv219_18626.txt', 'pos/cv220_29059.txt', 'pos/cv221_2695.txt', 'pos/cv222_17395.txt', 'pos/cv223_29066.txt', 'pos/cv224_17661.txt', 'pos/cv225_29224.txt', 'pos/cv226_2618.txt', 'pos/cv227_24215.txt', 'pos/cv228_5806.txt', 'pos/cv229_13611.txt', 'pos/cv230_7428.txt', 'pos/cv231_10425.txt', 'pos/cv232_14991.txt', 'pos/cv233_15964.txt', 'pos/cv234_20643.txt', 'pos/cv235_10217.txt', 'pos/cv236_11565.txt', 'pos/cv237_19221.txt', 'pos/cv238_12931.txt', 'pos/cv239_3385.txt', 'pos/cv240_14336.txt', 'pos/cv241_23130.txt', 'pos/cv242_10638.txt', 'pos/cv243_20728.txt', 'pos/cv244_21649.txt', 'pos/cv245_8569.txt', 'pos/cv246_28807.txt', 'pos/cv247_13142.txt', 'pos/cv248_13987.txt', 'pos/cv249_11640.txt', 'pos/cv250_25616.txt', 'pos/cv251_22636.txt', 'pos/cv252_23779.txt', 'pos/cv253_10077.txt', 'pos/cv254_6027.txt', 'pos/cv255_13683.txt', 'pos/cv256_14740.txt', 'pos/cv257_10975.txt', 'pos/cv258_5792.txt', 'pos/cv259_10934.txt', 'pos/cv260_13959.txt', 'pos/cv261_10954.txt', 'pos/cv262_12649.txt', 'pos/cv263_19259.txt', 'pos/cv264_12801.txt', 'pos/cv265_10814.txt', 'pos/cv266_25779.txt', 'pos/cv267_14952.txt', 'pos/cv268_18834.txt', 'pos/cv269_21732.txt', 'pos/cv270_6079.txt', 'pos/cv271_13837.txt', 'pos/cv272_18974.txt', 'pos/cv273_29112.txt', 'pos/cv274_25253.txt', 'pos/cv275_28887.txt', 'pos/cv276_15684.txt', 'pos/cv277_19091.txt', 'pos/cv278_13041.txt', 'pos/cv279_18329.txt', 'pos/cv280_8267.txt', 'pos/cv281_23253.txt', 'pos/cv282_6653.txt', 'pos/cv283_11055.txt', 'pos/cv284_19119.txt', 'pos/cv285_16494.txt', 'pos/cv286_25050.txt', 'pos/cv287_15900.txt', 'pos/cv288_18791.txt', 'pos/cv289_6463.txt', 'pos/cv290_11084.txt', 'pos/cv291_26635.txt', 'pos/cv292_7282.txt', 'pos/cv293_29856.txt', 'pos/cv294_11684.txt', 'pos/cv295_15570.txt', 'pos/cv296_12251.txt', 'pos/cv297_10047.txt', 'pos/cv298_23111.txt', 'pos/cv299_16214.txt', 'pos/cv300_22284.txt', 'pos/cv301_12146.txt', 'pos/cv302_25649.txt', 'pos/cv303_27520.txt', 'pos/cv304_28706.txt', 'pos/cv305_9946.txt', 'pos/cv306_10364.txt', 'pos/cv307_25270.txt', 'pos/cv308_5016.txt', 'pos/cv309_22571.txt', 'pos/cv310_13091.txt', 'pos/cv311_16002.txt', 'pos/cv312_29377.txt', 'pos/cv313_18198.txt', 'pos/cv314_14422.txt', 'pos/cv315_11629.txt', 'pos/cv316_6370.txt', 'pos/cv317_24049.txt', 'pos/cv318_10493.txt', 'pos/cv319_14727.txt', 'pos/cv320_9530.txt', 'pos/cv321_12843.txt', 'pos/cv322_20318.txt', 'pos/cv323_29805.txt', 'pos/cv324_7082.txt', 'pos/cv325_16629.txt', 'pos/cv326_13295.txt', 'pos/cv327_20292.txt', 'pos/cv328_10373.txt', 'pos/cv329_29370.txt', 'pos/cv330_29809.txt', 'pos/cv331_8273.txt', 'pos/cv332_16307.txt', 'pos/cv333_8916.txt', 'pos/cv334_10001.txt', 'pos/cv335_14665.txt', 'pos/cv336_10143.txt', 'pos/cv337_29181.txt', 'pos/cv338_8821.txt', 'pos/cv339_21119.txt', 'pos/cv340_13287.txt', 'pos/cv341_24430.txt', 'pos/cv342_19456.txt', 'pos/cv343_10368.txt', 'pos/cv344_5312.txt', 'pos/cv345_9954.txt', 'pos/cv346_18168.txt', 'pos/cv347_13194.txt', 'pos/cv348_18176.txt', 'pos/cv349_13507.txt', 'pos/cv350_20670.txt', 'pos/cv351_15458.txt', 'pos/cv352_5524.txt', 'pos/cv353_18159.txt', 'pos/cv354_8132.txt', 'pos/cv355_16413.txt', 'pos/cv356_25163.txt', 'pos/cv357_13156.txt', 'pos/cv358_10691.txt', 'pos/cv359_6647.txt', 'pos/cv360_8398.txt', 'pos/cv361_28944.txt', 'pos/cv362_15341.txt', 'pos/cv363_29332.txt', 'pos/cv364_12901.txt', 'pos/cv365_11576.txt', 'pos/cv366_10221.txt', 'pos/cv367_22792.txt', 'pos/cv368_10466.txt', 'pos/cv369_12886.txt', 'pos/cv370_5221.txt', 'pos/cv371_7630.txt', 'pos/cv372_6552.txt', 'pos/cv373_20404.txt', 'pos/cv374_25436.txt', 'pos/cv375_9929.txt', 'pos/cv376_19435.txt', 'pos/cv377_7946.txt', 'pos/cv378_20629.txt', 'pos/cv379_21963.txt', 'pos/cv380_7574.txt', 'pos/cv381_20172.txt', 'pos/cv382_7897.txt', 'pos/cv383_13116.txt', 'pos/cv384_17140.txt', 'pos/cv385_29741.txt', 'pos/cv386_10080.txt', 'pos/cv387_11507.txt', 'pos/cv388_12009.txt', 'pos/cv389_9369.txt', 'pos/cv390_11345.txt', 'pos/cv391_10802.txt', 'pos/cv392_11458.txt', 'pos/cv393_29327.txt', 'pos/cv394_5137.txt', 'pos/cv395_10849.txt', 'pos/cv396_17989.txt', 'pos/cv397_29023.txt', 'pos/cv398_15537.txt', 'pos/cv399_2877.txt', 'pos/cv400_19220.txt', 'pos/cv401_12605.txt', 'pos/cv402_14425.txt', 'pos/cv403_6621.txt', 'pos/cv404_20315.txt', 'pos/cv405_20399.txt', 'pos/cv406_21020.txt', 'pos/cv407_22637.txt', 'pos/cv408_5297.txt', 'pos/cv409_29786.txt', 'pos/cv410_24266.txt', 'pos/cv411_15007.txt', 'pos/cv412_24095.txt', 'pos/cv413_7398.txt', 'pos/cv414_10518.txt', 'pos/cv415_22517.txt', 'pos/cv416_11136.txt', 'pos/cv417_13115.txt', 'pos/cv418_14774.txt', 'pos/cv419_13394.txt', 'pos/cv420_28795.txt', 'pos/cv421_9709.txt', 'pos/cv422_9381.txt', 'pos/cv423_11155.txt', 'pos/cv424_8831.txt', 'pos/cv425_8250.txt', 'pos/cv426_10421.txt', 'pos/cv427_10825.txt', 'pos/cv428_11347.txt', 'pos/cv429_7439.txt', 'pos/cv430_17351.txt', 'pos/cv431_7085.txt', 'pos/cv432_14224.txt', 'pos/cv433_10144.txt', 'pos/cv434_5793.txt', 'pos/cv435_23110.txt', 'pos/cv436_19179.txt', 'pos/cv437_22849.txt', 'pos/cv438_8043.txt', 'pos/cv439_15970.txt', 'pos/cv440_15243.txt', 'pos/cv441_13711.txt', 'pos/cv442_13846.txt', 'pos/cv443_21118.txt', 'pos/cv444_9974.txt', 'pos/cv445_25882.txt', 'pos/cv446_11353.txt', 'pos/cv447_27332.txt', 'pos/cv448_14695.txt', 'pos/cv449_8785.txt', 'pos/cv450_7890.txt', 'pos/cv451_10690.txt', 'pos/cv452_5088.txt', 'pos/cv453_10379.txt', 'pos/cv454_2053.txt', 'pos/cv455_29000.txt', 'pos/cv456_18985.txt', 'pos/cv457_18453.txt', 'pos/cv458_8604.txt', 'pos/cv459_20319.txt', 'pos/cv460_10842.txt', 'pos/cv461_19600.txt', 'pos/cv462_19350.txt', 'pos/cv463_10343.txt', 'pos/cv464_15650.txt', 'pos/cv465_22431.txt', 'pos/cv466_18722.txt', 'pos/cv467_25773.txt', 'pos/cv468_15228.txt', 'pos/cv469_20630.txt', 'pos/cv470_15952.txt', 'pos/cv471_16858.txt', 'pos/cv472_29280.txt', 'pos/cv473_7367.txt', 'pos/cv474_10209.txt', 'pos/cv475_21692.txt', 'pos/cv476_16856.txt', 'pos/cv477_22479.txt', 'pos/cv478_14309.txt', 'pos/cv479_5649.txt', 'pos/cv480_19817.txt', 'pos/cv481_7436.txt', 'pos/cv482_10580.txt', 'pos/cv483_16378.txt', 'pos/cv484_25054.txt', 'pos/cv485_26649.txt', 'pos/cv486_9799.txt', 'pos/cv487_10446.txt', 'pos/cv488_19856.txt', 'pos/cv489_17906.txt', 'pos/cv490_17872.txt', 'pos/cv491_12145.txt', 'pos/cv492_18271.txt', 'pos/cv493_12839.txt', 'pos/cv494_17389.txt', 'pos/cv495_14518.txt', 'pos/cv496_10530.txt', 'pos/cv497_26980.txt', 'pos/cv498_8832.txt', 'pos/cv499_10658.txt', 'pos/cv500_10251.txt', 'pos/cv501_11657.txt', 'pos/cv502_10406.txt', 'pos/cv503_10558.txt', 'pos/cv504_29243.txt', 'pos/cv505_12090.txt', 'pos/cv506_15956.txt', 'pos/cv507_9220.txt', 'pos/cv508_16006.txt', 'pos/cv509_15888.txt', 'pos/cv510_23360.txt', 'pos/cv511_10132.txt', 'pos/cv512_15965.txt', 'pos/cv513_6923.txt', 'pos/cv514_11187.txt', 'pos/cv515_17069.txt', 'pos/cv516_11172.txt', 'pos/cv517_19219.txt', 'pos/cv518_13331.txt', 'pos/cv519_14661.txt', 'pos/cv520_12295.txt', 'pos/cv521_15828.txt', 'pos/cv522_5583.txt', 'pos/cv523_16615.txt', 'pos/cv524_23627.txt', 'pos/cv525_16122.txt', 'pos/cv526_12083.txt', 'pos/cv527_10123.txt', 'pos/cv528_10822.txt', 'pos/cv529_10420.txt', 'pos/cv530_16212.txt', 'pos/cv531_26486.txt', 'pos/cv532_6522.txt', 'pos/cv533_9821.txt', 'pos/cv534_14083.txt', 'pos/cv535_19728.txt', 'pos/cv536_27134.txt', 'pos/cv537_12370.txt', 'pos/cv538_28667.txt', 'pos/cv539_20347.txt', 'pos/cv540_3421.txt', 'pos/cv541_28835.txt', 'pos/cv542_18980.txt', 'pos/cv543_5045.txt', 'pos/cv544_5108.txt', 'pos/cv545_12014.txt', 'pos/cv546_11767.txt', 'pos/cv547_16324.txt', 'pos/cv548_17731.txt', 'pos/cv549_21443.txt', 'pos/cv550_22211.txt', 'pos/cv551_10565.txt', 'pos/cv552_10016.txt', 'pos/cv553_26915.txt', 'pos/cv554_13151.txt', 'pos/cv555_23922.txt', 'pos/cv556_14808.txt', 'pos/cv557_11449.txt', 'pos/cv558_29507.txt', 'pos/cv559_0050.txt', 'pos/cv560_17175.txt', 'pos/cv561_9201.txt', 'pos/cv562_10359.txt', 'pos/cv563_17257.txt', 'pos/cv564_11110.txt', 'pos/cv565_29572.txt', 'pos/cv566_8581.txt', 'pos/cv567_29611.txt', 'pos/cv568_15638.txt', 'pos/cv569_26381.txt', 'pos/cv570_29082.txt', 'pos/cv571_29366.txt', 'pos/cv572_18657.txt', 'pos/cv573_29525.txt', 'pos/cv574_22156.txt', 'pos/cv575_21150.txt', 'pos/cv576_14094.txt', 'pos/cv577_28549.txt', 'pos/cv578_15094.txt', 'pos/cv579_11605.txt', 'pos/cv580_14064.txt', 'pos/cv581_19381.txt', 'pos/cv582_6559.txt', 'pos/cv583_29692.txt', 'pos/cv584_29722.txt', 'pos/cv585_22496.txt', 'pos/cv586_7543.txt', 'pos/cv587_19162.txt', 'pos/cv588_13008.txt', 'pos/cv589_12064.txt', 'pos/cv590_19290.txt', 'pos/cv591_23640.txt', 'pos/cv592_22315.txt', 'pos/cv593_10987.txt', 'pos/cv594_11039.txt', 'pos/cv595_25335.txt', 'pos/cv596_28311.txt', 'pos/cv597_26360.txt', 'pos/cv598_16452.txt', 'pos/cv599_20988.txt', 'pos/cv600_23878.txt', 'pos/cv601_23453.txt', 'pos/cv602_8300.txt', 'pos/cv603_17694.txt', 'pos/cv604_2230.txt', 'pos/cv605_11800.txt', 'pos/cv606_15985.txt', 'pos/cv607_7717.txt', 'pos/cv608_23231.txt', 'pos/cv609_23877.txt', 'pos/cv610_2287.txt', 'pos/cv611_21120.txt', 'pos/cv612_5461.txt', 'pos/cv613_21796.txt', 'pos/cv614_10626.txt', 'pos/cv615_14182.txt', 'pos/cv616_29319.txt', 'pos/cv617_9322.txt', 'pos/cv618_8974.txt', 'pos/cv619_12462.txt', 'pos/cv620_24265.txt', 'pos/cv621_14368.txt', 'pos/cv622_8147.txt', 'pos/cv623_15356.txt', 'pos/cv624_10744.txt', 'pos/cv625_12440.txt', 'pos/cv626_7410.txt', 'pos/cv627_11620.txt', 'pos/cv628_19325.txt', 'pos/cv629_14909.txt', 'pos/cv630_10057.txt', 'pos/cv631_4967.txt', 'pos/cv632_9610.txt', 'pos/cv633_29837.txt', 'pos/cv634_11101.txt', 'pos/cv635_10022.txt', 'pos/cv636_15279.txt', 'pos/cv637_1250.txt', 'pos/cv638_2953.txt', 'pos/cv639_10308.txt', 'pos/cv640_5378.txt', 'pos/cv641_12349.txt', 'pos/cv642_29867.txt', 'pos/cv643_29349.txt', 'pos/cv644_17154.txt', 'pos/cv645_15668.txt', 'pos/cv646_15065.txt', 'pos/cv647_13691.txt', 'pos/cv648_15792.txt', 'pos/cv649_12735.txt', 'pos/cv650_14340.txt', 'pos/cv651_10492.txt', 'pos/cv652_13972.txt', 'pos/cv653_19583.txt', 'pos/cv654_18246.txt', 'pos/cv655_11154.txt', 'pos/cv656_24201.txt', 'pos/cv657_24513.txt', 'pos/cv658_10532.txt', 'pos/cv659_19944.txt', 'pos/cv660_21893.txt', 'pos/cv661_2450.txt', 'pos/cv662_13320.txt', 'pos/cv663_13019.txt', 'pos/cv664_4389.txt', 'pos/cv665_29538.txt', 'pos/cv666_18963.txt', 'pos/cv667_18467.txt', 'pos/cv668_17604.txt', 'pos/cv669_22995.txt', 'pos/cv670_25826.txt', 'pos/cv671_5054.txt', 'pos/cv672_28083.txt', 'pos/cv673_24714.txt', 'pos/cv674_10732.txt', 'pos/cv675_21588.txt', 'pos/cv676_21090.txt', 'pos/cv677_17715.txt', 'pos/cv678_13419.txt', 'pos/cv679_28559.txt', 'pos/cv680_10160.txt', 'pos/cv681_9692.txt', 'pos/cv682_16139.txt', 'pos/cv683_12167.txt', 'pos/cv684_11798.txt', 'pos/cv685_5947.txt', 'pos/cv686_13900.txt', 'pos/cv687_21100.txt', 'pos/cv688_7368.txt', 'pos/cv689_12587.txt', 'pos/cv690_5619.txt', 'pos/cv691_5043.txt', 'pos/cv692_15451.txt', 'pos/cv693_18063.txt', 'pos/cv694_4876.txt', 'pos/cv695_21108.txt', 'pos/cv696_29740.txt', 'pos/cv697_11162.txt', 'pos/cv698_15253.txt', 'pos/cv699_7223.txt', 'pos/cv700_21947.txt', 'pos/cv701_14252.txt', 'pos/cv702_11500.txt', 'pos/cv703_16143.txt', 'pos/cv704_15969.txt', 'pos/cv705_11059.txt', 'pos/cv706_24716.txt', 'pos/cv707_10678.txt', 'pos/cv708_28729.txt', 'pos/cv709_10529.txt', 'pos/cv710_22577.txt', 'pos/cv711_11665.txt', 'pos/cv712_22920.txt', 'pos/cv713_29155.txt', 'pos/cv714_18502.txt', 'pos/cv715_18179.txt', 'pos/cv716_10514.txt', 'pos/cv717_15953.txt', 'pos/cv718_11434.txt', 'pos/cv719_5713.txt', 'pos/cv720_5389.txt', 'pos/cv721_29121.txt', 'pos/cv722_7110.txt', 'pos/cv723_8648.txt', 'pos/cv724_13681.txt', 'pos/cv725_10103.txt', 'pos/cv726_4719.txt', 'pos/cv727_4978.txt', 'pos/cv728_16133.txt', 'pos/cv729_10154.txt', 'pos/cv730_10279.txt', 'pos/cv731_4136.txt', 'pos/cv732_12245.txt', 'pos/cv733_9839.txt', 'pos/cv734_21568.txt', 'pos/cv735_18801.txt', 'pos/cv736_23670.txt', 'pos/cv737_28907.txt', 'pos/cv738_10116.txt', 'pos/cv739_11209.txt', 'pos/cv740_12445.txt', 'pos/cv741_11890.txt', 'pos/cv742_7751.txt', 'pos/cv743_15449.txt', 'pos/cv744_10038.txt', 'pos/cv745_12773.txt', 'pos/cv746_10147.txt', 'pos/cv747_16556.txt', 'pos/cv748_12786.txt', 'pos/cv749_17765.txt', 'pos/cv750_10180.txt', 'pos/cv751_15719.txt', 'pos/cv752_24155.txt', 'pos/cv753_10875.txt', 'pos/cv754_7216.txt', 'pos/cv755_23616.txt', 'pos/cv756_22540.txt', 'pos/cv757_10189.txt', 'pos/cv758_9671.txt', 'pos/cv759_13522.txt', 'pos/cv760_8597.txt', 'pos/cv761_12620.txt', 'pos/cv762_13927.txt', 'pos/cv763_14729.txt', 'pos/cv764_11739.txt', 'pos/cv765_19037.txt', 'pos/cv766_7540.txt', 'pos/cv767_14062.txt', 'pos/cv768_11751.txt', 'pos/cv769_8123.txt', 'pos/cv770_10451.txt', 'pos/cv771_28665.txt', 'pos/cv772_12119.txt', 'pos/cv773_18817.txt', 'pos/cv774_13845.txt', 'pos/cv775_16237.txt', 'pos/cv776_20529.txt', 'pos/cv777_10094.txt', 'pos/cv778_17330.txt', 'pos/cv779_17881.txt', 'pos/cv780_7984.txt', 'pos/cv781_5262.txt', 'pos/cv782_19526.txt', 'pos/cv783_13227.txt', 'pos/cv784_14394.txt', 'pos/cv785_22600.txt', 'pos/cv786_22497.txt', 'pos/cv787_13743.txt', 'pos/cv788_25272.txt', 'pos/cv789_12136.txt', 'pos/cv790_14600.txt', 'pos/cv791_16302.txt', 'pos/cv792_3832.txt', 'pos/cv793_13650.txt', 'pos/cv794_15868.txt', 'pos/cv795_10122.txt', 'pos/cv796_15782.txt', 'pos/cv797_6957.txt', 'pos/cv798_23531.txt', 'pos/cv799_18543.txt', 'pos/cv800_12368.txt', 'pos/cv801_25228.txt', 'pos/cv802_28664.txt', 'pos/cv803_8207.txt', 'pos/cv804_10862.txt', 'pos/cv805_19601.txt', 'pos/cv806_8842.txt', 'pos/cv807_21740.txt', 'pos/cv808_12635.txt', 'pos/cv809_5009.txt', 'pos/cv810_12458.txt', 'pos/cv811_21386.txt', 'pos/cv812_17924.txt', 'pos/cv813_6534.txt', 'pos/cv814_18975.txt', 'pos/cv815_22456.txt', 'pos/cv816_13655.txt', 'pos/cv817_4041.txt', 'pos/cv818_10211.txt', 'pos/cv819_9364.txt', 'pos/cv820_22892.txt', 'pos/cv821_29364.txt', 'pos/cv822_20049.txt', 'pos/cv823_15569.txt', 'pos/cv824_8838.txt', 'pos/cv825_5063.txt', 'pos/cv826_11834.txt', 'pos/cv827_18331.txt', 'pos/cv828_19831.txt', 'pos/cv829_20289.txt', 'pos/cv830_6014.txt', 'pos/cv831_14689.txt', 'pos/cv832_23275.txt', 'pos/cv833_11053.txt', 'pos/cv834_22195.txt', 'pos/cv835_19159.txt', 'pos/cv836_12968.txt', 'pos/cv837_27325.txt', 'pos/cv838_24728.txt', 'pos/cv839_21467.txt', 'pos/cv840_16321.txt', 'pos/cv841_3967.txt', 'pos/cv842_5866.txt', 'pos/cv843_15544.txt', 'pos/cv844_12690.txt', 'pos/cv845_14290.txt', 'pos/cv846_29497.txt', 'pos/cv847_1941.txt', 'pos/cv848_10036.txt', 'pos/cv849_15729.txt', 'pos/cv850_16466.txt', 'pos/cv851_20469.txt', 'pos/cv852_27523.txt', 'pos/cv853_29233.txt', 'pos/cv854_17740.txt', 'pos/cv855_20661.txt', 'pos/cv856_29013.txt', 'pos/cv857_15958.txt', 'pos/cv858_18819.txt', 'pos/cv859_14107.txt', 'pos/cv860_13853.txt', 'pos/cv861_1198.txt', 'pos/cv862_14324.txt', 'pos/cv863_7424.txt', 'pos/cv864_3416.txt', 'pos/cv865_2895.txt', 'pos/cv866_29691.txt', 'pos/cv867_16661.txt', 'pos/cv868_11948.txt', 'pos/cv869_23611.txt', 'pos/cv870_16348.txt', 'pos/cv871_24888.txt', 'pos/cv872_12591.txt', 'pos/cv873_18636.txt', 'pos/cv874_11236.txt', 'pos/cv875_5754.txt', 'pos/cv876_9390.txt', 'pos/cv877_29274.txt', 'pos/cv878_15694.txt', 'pos/cv879_14903.txt', 'pos/cv880_29800.txt', 'pos/cv881_13254.txt', 'pos/cv882_10026.txt', 'pos/cv883_27751.txt', 'pos/cv884_13632.txt', 'pos/cv885_12318.txt', 'pos/cv886_18177.txt', 'pos/cv887_5126.txt', 'pos/cv888_24435.txt', 'pos/cv889_21430.txt', 'pos/cv890_3977.txt', 'pos/cv891_6385.txt', 'pos/cv892_17576.txt', 'pos/cv893_26269.txt', 'pos/cv894_2068.txt', 'pos/cv895_21022.txt', 'pos/cv896_16071.txt', 'pos/cv897_10837.txt', 'pos/cv898_14187.txt', 'pos/cv899_16014.txt', 'pos/cv900_10331.txt', 'pos/cv901_11017.txt', 'pos/cv902_12256.txt', 'pos/cv903_17822.txt', 'pos/cv904_24353.txt', 'pos/cv905_29114.txt', 'pos/cv906_11491.txt', 'pos/cv907_3541.txt', 'pos/cv908_16009.txt', 'pos/cv909_9960.txt', 'pos/cv910_20488.txt', 'pos/cv911_20260.txt', 'pos/cv912_5674.txt', 'pos/cv913_29252.txt', 'pos/cv914_28742.txt', 'pos/cv915_8841.txt', 'pos/cv916_15467.txt', 'pos/cv917_29715.txt', 'pos/cv918_2693.txt', 'pos/cv919_16380.txt', 'pos/cv920_29622.txt', 'pos/cv921_12747.txt', 'pos/cv922_10073.txt', 'pos/cv923_11051.txt', 'pos/cv924_29540.txt', 'pos/cv925_8969.txt', 'pos/cv926_17059.txt', 'pos/cv927_10681.txt', 'pos/cv928_9168.txt', 'pos/cv929_16908.txt', 'pos/cv930_13475.txt', 'pos/cv931_17563.txt', 'pos/cv932_13401.txt', 'pos/cv933_23776.txt', 'pos/cv934_19027.txt', 'pos/cv935_23841.txt', 'pos/cv936_15954.txt', 'pos/cv937_9811.txt', 'pos/cv938_10220.txt', 'pos/cv939_10583.txt', 'pos/cv940_17705.txt', 'pos/cv941_10246.txt', 'pos/cv942_17082.txt', 'pos/cv943_22488.txt', 'pos/cv944_13521.txt', 'pos/cv945_12160.txt', 'pos/cv946_18658.txt', 'pos/cv947_10601.txt', 'pos/cv948_24606.txt', 'pos/cv949_20112.txt', 'pos/cv950_12350.txt', 'pos/cv951_10926.txt', 'pos/cv952_25240.txt', 'pos/cv953_6836.txt', 'pos/cv954_18628.txt', 'pos/cv955_25001.txt', 'pos/cv956_11609.txt', 'pos/cv957_8737.txt', 'pos/cv958_12162.txt', 'pos/cv959_14611.txt', 'pos/cv960_29007.txt', 'pos/cv961_5682.txt', 'pos/cv962_9803.txt', 'pos/cv963_6895.txt', 'pos/cv964_6021.txt', 'pos/cv965_26071.txt', 'pos/cv966_28832.txt', 'pos/cv967_5788.txt', 'pos/cv968_24218.txt', 'pos/cv969_13250.txt', 'pos/cv970_18450.txt', 'pos/cv971_10874.txt', 'pos/cv972_26417.txt', 'pos/cv973_10066.txt', 'pos/cv974_22941.txt', 'pos/cv975_10981.txt', 'pos/cv976_10267.txt', 'pos/cv977_4938.txt', 'pos/cv978_20929.txt', 'pos/cv979_18921.txt', 'pos/cv980_10953.txt', 'pos/cv981_14989.txt', 'pos/cv982_21103.txt', 'pos/cv983_22928.txt', 'pos/cv984_12767.txt', 'pos/cv985_6359.txt', 'pos/cv986_13527.txt', 'pos/cv987_6965.txt', 'pos/cv988_18740.txt', 'pos/cv989_15824.txt', 'pos/cv990_11591.txt', 'pos/cv991_18645.txt', 'pos/cv992_11962.txt', 'pos/cv993_29737.txt', 'pos/cv994_12270.txt', 'pos/cv995_21821.txt', 'pos/cv996_11592.txt', 'pos/cv997_5046.txt', 'pos/cv998_14111.txt', 'pos/cv999_13106.txt']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.fileids('pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_rev = movie_reviews.fileids('neg')\n",
    "len(neg_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt',\n",
       " 'neg/cv005_29357.txt',\n",
       " 'neg/cv006_17022.txt',\n",
       " 'neg/cv007_4992.txt',\n",
       " 'neg/cv008_29326.txt',\n",
       " 'neg/cv009_29417.txt',\n",
       " 'neg/cv010_29063.txt',\n",
       " 'neg/cv011_13044.txt',\n",
       " 'neg/cv012_29411.txt',\n",
       " 'neg/cv013_10494.txt',\n",
       " 'neg/cv014_15600.txt',\n",
       " 'neg/cv015_29356.txt',\n",
       " 'neg/cv016_4348.txt',\n",
       " 'neg/cv017_23487.txt',\n",
       " 'neg/cv018_21672.txt',\n",
       " 'neg/cv019_16117.txt',\n",
       " 'neg/cv020_9234.txt',\n",
       " 'neg/cv021_17313.txt',\n",
       " 'neg/cv022_14227.txt',\n",
       " 'neg/cv023_13847.txt',\n",
       " 'neg/cv024_7033.txt',\n",
       " 'neg/cv025_29825.txt',\n",
       " 'neg/cv026_29229.txt',\n",
       " 'neg/cv027_26270.txt',\n",
       " 'neg/cv028_26964.txt',\n",
       " 'neg/cv029_19943.txt',\n",
       " 'neg/cv030_22893.txt',\n",
       " 'neg/cv031_19540.txt',\n",
       " 'neg/cv032_23718.txt',\n",
       " 'neg/cv033_25680.txt',\n",
       " 'neg/cv034_29446.txt',\n",
       " 'neg/cv035_3343.txt',\n",
       " 'neg/cv036_18385.txt',\n",
       " 'neg/cv037_19798.txt',\n",
       " 'neg/cv038_9781.txt',\n",
       " 'neg/cv039_5963.txt',\n",
       " 'neg/cv040_8829.txt',\n",
       " 'neg/cv041_22364.txt',\n",
       " 'neg/cv042_11927.txt',\n",
       " 'neg/cv043_16808.txt',\n",
       " 'neg/cv044_18429.txt',\n",
       " 'neg/cv045_25077.txt',\n",
       " 'neg/cv046_10613.txt',\n",
       " 'neg/cv047_18725.txt',\n",
       " 'neg/cv048_18380.txt',\n",
       " 'neg/cv049_21917.txt',\n",
       " 'neg/cv050_12128.txt',\n",
       " 'neg/cv051_10751.txt',\n",
       " 'neg/cv052_29318.txt',\n",
       " 'neg/cv053_23117.txt',\n",
       " 'neg/cv054_4101.txt',\n",
       " 'neg/cv055_8926.txt',\n",
       " 'neg/cv056_14663.txt',\n",
       " 'neg/cv057_7962.txt',\n",
       " 'neg/cv058_8469.txt',\n",
       " 'neg/cv059_28723.txt',\n",
       " 'neg/cv060_11754.txt',\n",
       " 'neg/cv061_9321.txt',\n",
       " 'neg/cv062_24556.txt',\n",
       " 'neg/cv063_28852.txt',\n",
       " 'neg/cv064_25842.txt',\n",
       " 'neg/cv065_16909.txt',\n",
       " 'neg/cv066_11668.txt',\n",
       " 'neg/cv067_21192.txt',\n",
       " 'neg/cv068_14810.txt',\n",
       " 'neg/cv069_11613.txt',\n",
       " 'neg/cv070_13249.txt',\n",
       " 'neg/cv071_12969.txt',\n",
       " 'neg/cv072_5928.txt',\n",
       " 'neg/cv073_23039.txt',\n",
       " 'neg/cv074_7188.txt',\n",
       " 'neg/cv075_6250.txt',\n",
       " 'neg/cv076_26009.txt',\n",
       " 'neg/cv077_23172.txt',\n",
       " 'neg/cv078_16506.txt',\n",
       " 'neg/cv079_12766.txt',\n",
       " 'neg/cv080_14899.txt',\n",
       " 'neg/cv081_18241.txt',\n",
       " 'neg/cv082_11979.txt',\n",
       " 'neg/cv083_25491.txt',\n",
       " 'neg/cv084_15183.txt',\n",
       " 'neg/cv085_15286.txt',\n",
       " 'neg/cv086_19488.txt',\n",
       " 'neg/cv087_2145.txt',\n",
       " 'neg/cv088_25274.txt',\n",
       " 'neg/cv089_12222.txt',\n",
       " 'neg/cv090_0049.txt',\n",
       " 'neg/cv091_7899.txt',\n",
       " 'neg/cv092_27987.txt',\n",
       " 'neg/cv093_15606.txt',\n",
       " 'neg/cv094_27868.txt',\n",
       " 'neg/cv095_28730.txt',\n",
       " 'neg/cv096_12262.txt',\n",
       " 'neg/cv097_26081.txt',\n",
       " 'neg/cv098_17021.txt',\n",
       " 'neg/cv099_11189.txt',\n",
       " 'neg/cv100_12406.txt',\n",
       " 'neg/cv101_10537.txt',\n",
       " 'neg/cv102_8306.txt',\n",
       " 'neg/cv103_11943.txt',\n",
       " 'neg/cv104_19176.txt',\n",
       " 'neg/cv105_19135.txt',\n",
       " 'neg/cv106_18379.txt',\n",
       " 'neg/cv107_25639.txt',\n",
       " 'neg/cv108_17064.txt',\n",
       " 'neg/cv109_22599.txt',\n",
       " 'neg/cv110_27832.txt',\n",
       " 'neg/cv111_12253.txt',\n",
       " 'neg/cv112_12178.txt',\n",
       " 'neg/cv113_24354.txt',\n",
       " 'neg/cv114_19501.txt',\n",
       " 'neg/cv115_26443.txt',\n",
       " 'neg/cv116_28734.txt',\n",
       " 'neg/cv117_25625.txt',\n",
       " 'neg/cv118_28837.txt',\n",
       " 'neg/cv119_9909.txt',\n",
       " 'neg/cv120_3793.txt',\n",
       " 'neg/cv121_18621.txt',\n",
       " 'neg/cv122_7891.txt',\n",
       " 'neg/cv123_12165.txt',\n",
       " 'neg/cv124_3903.txt',\n",
       " 'neg/cv125_9636.txt',\n",
       " 'neg/cv126_28821.txt',\n",
       " 'neg/cv127_16451.txt',\n",
       " 'neg/cv128_29444.txt',\n",
       " 'neg/cv129_18373.txt',\n",
       " 'neg/cv130_18521.txt',\n",
       " 'neg/cv131_11568.txt',\n",
       " 'neg/cv132_5423.txt',\n",
       " 'neg/cv133_18065.txt',\n",
       " 'neg/cv134_23300.txt',\n",
       " 'neg/cv135_12506.txt',\n",
       " 'neg/cv136_12384.txt',\n",
       " 'neg/cv137_17020.txt',\n",
       " 'neg/cv138_13903.txt',\n",
       " 'neg/cv139_14236.txt',\n",
       " 'neg/cv140_7963.txt',\n",
       " 'neg/cv141_17179.txt',\n",
       " 'neg/cv142_23657.txt',\n",
       " 'neg/cv143_21158.txt',\n",
       " 'neg/cv144_5010.txt',\n",
       " 'neg/cv145_12239.txt',\n",
       " 'neg/cv146_19587.txt',\n",
       " 'neg/cv147_22625.txt',\n",
       " 'neg/cv148_18084.txt',\n",
       " 'neg/cv149_17084.txt',\n",
       " 'neg/cv150_14279.txt',\n",
       " 'neg/cv151_17231.txt',\n",
       " 'neg/cv152_9052.txt',\n",
       " 'neg/cv153_11607.txt',\n",
       " 'neg/cv154_9562.txt',\n",
       " 'neg/cv155_7845.txt',\n",
       " 'neg/cv156_11119.txt',\n",
       " 'neg/cv157_29302.txt',\n",
       " 'neg/cv158_10914.txt',\n",
       " 'neg/cv159_29374.txt',\n",
       " 'neg/cv160_10848.txt',\n",
       " 'neg/cv161_12224.txt',\n",
       " 'neg/cv162_10977.txt',\n",
       " 'neg/cv163_10110.txt',\n",
       " 'neg/cv164_23451.txt',\n",
       " 'neg/cv165_2389.txt',\n",
       " 'neg/cv166_11959.txt',\n",
       " 'neg/cv167_18094.txt',\n",
       " 'neg/cv168_7435.txt',\n",
       " 'neg/cv169_24973.txt',\n",
       " 'neg/cv170_29808.txt',\n",
       " 'neg/cv171_15164.txt',\n",
       " 'neg/cv172_12037.txt',\n",
       " 'neg/cv173_4295.txt',\n",
       " 'neg/cv174_9735.txt',\n",
       " 'neg/cv175_7375.txt',\n",
       " 'neg/cv176_14196.txt',\n",
       " 'neg/cv177_10904.txt',\n",
       " 'neg/cv178_14380.txt',\n",
       " 'neg/cv179_9533.txt',\n",
       " 'neg/cv180_17823.txt',\n",
       " 'neg/cv181_16083.txt',\n",
       " 'neg/cv182_7791.txt',\n",
       " 'neg/cv183_19826.txt',\n",
       " 'neg/cv184_26935.txt',\n",
       " 'neg/cv185_28372.txt',\n",
       " 'neg/cv186_2396.txt',\n",
       " 'neg/cv187_14112.txt',\n",
       " 'neg/cv188_20687.txt',\n",
       " 'neg/cv189_24248.txt',\n",
       " 'neg/cv190_27176.txt',\n",
       " 'neg/cv191_29539.txt',\n",
       " 'neg/cv192_16079.txt',\n",
       " 'neg/cv193_5393.txt',\n",
       " 'neg/cv194_12855.txt',\n",
       " 'neg/cv195_16146.txt',\n",
       " 'neg/cv196_28898.txt',\n",
       " 'neg/cv197_29271.txt',\n",
       " 'neg/cv198_19313.txt',\n",
       " 'neg/cv199_9721.txt',\n",
       " 'neg/cv200_29006.txt',\n",
       " 'neg/cv201_7421.txt',\n",
       " 'neg/cv202_11382.txt',\n",
       " 'neg/cv203_19052.txt',\n",
       " 'neg/cv204_8930.txt',\n",
       " 'neg/cv205_9676.txt',\n",
       " 'neg/cv206_15893.txt',\n",
       " 'neg/cv207_29141.txt',\n",
       " 'neg/cv208_9475.txt',\n",
       " 'neg/cv209_28973.txt',\n",
       " 'neg/cv210_9557.txt',\n",
       " 'neg/cv211_9955.txt',\n",
       " 'neg/cv212_10054.txt',\n",
       " 'neg/cv213_20300.txt',\n",
       " 'neg/cv214_13285.txt',\n",
       " 'neg/cv215_23246.txt',\n",
       " 'neg/cv216_20165.txt',\n",
       " 'neg/cv217_28707.txt',\n",
       " 'neg/cv218_25651.txt',\n",
       " 'neg/cv219_19874.txt',\n",
       " 'neg/cv220_28906.txt',\n",
       " 'neg/cv221_27081.txt',\n",
       " 'neg/cv222_18720.txt',\n",
       " 'neg/cv223_28923.txt',\n",
       " 'neg/cv224_18875.txt',\n",
       " 'neg/cv225_29083.txt',\n",
       " 'neg/cv226_26692.txt',\n",
       " 'neg/cv227_25406.txt',\n",
       " 'neg/cv228_5644.txt',\n",
       " 'neg/cv229_15200.txt',\n",
       " 'neg/cv230_7913.txt',\n",
       " 'neg/cv231_11028.txt',\n",
       " 'neg/cv232_16768.txt',\n",
       " 'neg/cv233_17614.txt',\n",
       " 'neg/cv234_22123.txt',\n",
       " 'neg/cv235_10704.txt',\n",
       " 'neg/cv236_12427.txt',\n",
       " 'neg/cv237_20635.txt',\n",
       " 'neg/cv238_14285.txt',\n",
       " 'neg/cv239_29828.txt',\n",
       " 'neg/cv240_15948.txt',\n",
       " 'neg/cv241_24602.txt',\n",
       " 'neg/cv242_11354.txt',\n",
       " 'neg/cv243_22164.txt',\n",
       " 'neg/cv244_22935.txt',\n",
       " 'neg/cv245_8938.txt',\n",
       " 'neg/cv246_28668.txt',\n",
       " 'neg/cv247_14668.txt',\n",
       " 'neg/cv248_15672.txt',\n",
       " 'neg/cv249_12674.txt',\n",
       " 'neg/cv250_26462.txt',\n",
       " 'neg/cv251_23901.txt',\n",
       " 'neg/cv252_24974.txt',\n",
       " 'neg/cv253_10190.txt',\n",
       " 'neg/cv254_5870.txt',\n",
       " 'neg/cv255_15267.txt',\n",
       " 'neg/cv256_16529.txt',\n",
       " 'neg/cv257_11856.txt',\n",
       " 'neg/cv258_5627.txt',\n",
       " 'neg/cv259_11827.txt',\n",
       " 'neg/cv260_15652.txt',\n",
       " 'neg/cv261_11855.txt',\n",
       " 'neg/cv262_13812.txt',\n",
       " 'neg/cv263_20693.txt',\n",
       " 'neg/cv264_14108.txt',\n",
       " 'neg/cv265_11625.txt',\n",
       " 'neg/cv266_26644.txt',\n",
       " 'neg/cv267_16618.txt',\n",
       " 'neg/cv268_20288.txt',\n",
       " 'neg/cv269_23018.txt',\n",
       " 'neg/cv270_5873.txt',\n",
       " 'neg/cv271_15364.txt',\n",
       " 'neg/cv272_20313.txt',\n",
       " 'neg/cv273_28961.txt',\n",
       " 'neg/cv274_26379.txt',\n",
       " 'neg/cv275_28725.txt',\n",
       " 'neg/cv276_17126.txt',\n",
       " 'neg/cv277_20467.txt',\n",
       " 'neg/cv278_14533.txt',\n",
       " 'neg/cv279_19452.txt',\n",
       " 'neg/cv280_8651.txt',\n",
       " 'neg/cv281_24711.txt',\n",
       " 'neg/cv282_6833.txt',\n",
       " 'neg/cv283_11963.txt',\n",
       " 'neg/cv284_20530.txt',\n",
       " 'neg/cv285_18186.txt',\n",
       " 'neg/cv286_26156.txt',\n",
       " 'neg/cv287_17410.txt',\n",
       " 'neg/cv288_20212.txt',\n",
       " 'neg/cv289_6239.txt',\n",
       " 'neg/cv290_11981.txt',\n",
       " 'neg/cv291_26844.txt',\n",
       " 'neg/cv292_7804.txt',\n",
       " 'neg/cv293_29731.txt',\n",
       " 'neg/cv294_12695.txt',\n",
       " 'neg/cv295_17060.txt',\n",
       " 'neg/cv296_13146.txt',\n",
       " 'neg/cv297_10104.txt',\n",
       " 'neg/cv298_24487.txt',\n",
       " 'neg/cv299_17950.txt',\n",
       " 'neg/cv300_23302.txt',\n",
       " 'neg/cv301_13010.txt',\n",
       " 'neg/cv302_26481.txt',\n",
       " 'neg/cv303_27366.txt',\n",
       " 'neg/cv304_28489.txt',\n",
       " 'neg/cv305_9937.txt',\n",
       " 'neg/cv306_10859.txt',\n",
       " 'neg/cv307_26382.txt',\n",
       " 'neg/cv308_5079.txt',\n",
       " 'neg/cv309_23737.txt',\n",
       " 'neg/cv310_14568.txt',\n",
       " 'neg/cv311_17708.txt',\n",
       " 'neg/cv312_29308.txt',\n",
       " 'neg/cv313_19337.txt',\n",
       " 'neg/cv314_16095.txt',\n",
       " 'neg/cv315_12638.txt',\n",
       " 'neg/cv316_5972.txt',\n",
       " 'neg/cv317_25111.txt',\n",
       " 'neg/cv318_11146.txt',\n",
       " 'neg/cv319_16459.txt',\n",
       " 'neg/cv320_9693.txt',\n",
       " 'neg/cv321_14191.txt',\n",
       " 'neg/cv322_21820.txt',\n",
       " 'neg/cv323_29633.txt',\n",
       " 'neg/cv324_7502.txt',\n",
       " 'neg/cv325_18330.txt',\n",
       " 'neg/cv326_14777.txt',\n",
       " 'neg/cv327_21743.txt',\n",
       " 'neg/cv328_10908.txt',\n",
       " 'neg/cv329_29293.txt',\n",
       " 'neg/cv330_29675.txt',\n",
       " 'neg/cv331_8656.txt',\n",
       " 'neg/cv332_17997.txt',\n",
       " 'neg/cv333_9443.txt',\n",
       " 'neg/cv334_0074.txt',\n",
       " 'neg/cv335_16299.txt',\n",
       " 'neg/cv336_10363.txt',\n",
       " 'neg/cv337_29061.txt',\n",
       " 'neg/cv338_9183.txt',\n",
       " 'neg/cv339_22452.txt',\n",
       " 'neg/cv340_14776.txt',\n",
       " 'neg/cv341_25667.txt',\n",
       " 'neg/cv342_20917.txt',\n",
       " 'neg/cv343_10906.txt',\n",
       " 'neg/cv344_5376.txt',\n",
       " 'neg/cv345_9966.txt',\n",
       " 'neg/cv346_19198.txt',\n",
       " 'neg/cv347_14722.txt',\n",
       " 'neg/cv348_19207.txt',\n",
       " 'neg/cv349_15032.txt',\n",
       " 'neg/cv350_22139.txt',\n",
       " 'neg/cv351_17029.txt',\n",
       " 'neg/cv352_5414.txt',\n",
       " 'neg/cv353_19197.txt',\n",
       " 'neg/cv354_8573.txt',\n",
       " 'neg/cv355_18174.txt',\n",
       " 'neg/cv356_26170.txt',\n",
       " 'neg/cv357_14710.txt',\n",
       " 'neg/cv358_11557.txt',\n",
       " 'neg/cv359_6751.txt',\n",
       " 'neg/cv360_8927.txt',\n",
       " 'neg/cv361_28738.txt',\n",
       " 'neg/cv362_16985.txt',\n",
       " 'neg/cv363_29273.txt',\n",
       " 'neg/cv364_14254.txt',\n",
       " 'neg/cv365_12442.txt',\n",
       " 'neg/cv366_10709.txt',\n",
       " 'neg/cv367_24065.txt',\n",
       " 'neg/cv368_11090.txt',\n",
       " 'neg/cv369_14245.txt',\n",
       " 'neg/cv370_5338.txt',\n",
       " 'neg/cv371_8197.txt',\n",
       " 'neg/cv372_6654.txt',\n",
       " 'neg/cv373_21872.txt',\n",
       " 'neg/cv374_26455.txt',\n",
       " 'neg/cv375_9932.txt',\n",
       " 'neg/cv376_20883.txt',\n",
       " 'neg/cv377_8440.txt',\n",
       " 'neg/cv378_21982.txt',\n",
       " 'neg/cv379_23167.txt',\n",
       " 'neg/cv380_8164.txt',\n",
       " 'neg/cv381_21673.txt',\n",
       " 'neg/cv382_8393.txt',\n",
       " 'neg/cv383_14662.txt',\n",
       " 'neg/cv384_18536.txt',\n",
       " 'neg/cv385_29621.txt',\n",
       " 'neg/cv386_10229.txt',\n",
       " 'neg/cv387_12391.txt',\n",
       " 'neg/cv388_12810.txt',\n",
       " 'neg/cv389_9611.txt',\n",
       " 'neg/cv390_12187.txt',\n",
       " 'neg/cv391_11615.txt',\n",
       " 'neg/cv392_12238.txt',\n",
       " 'neg/cv393_29234.txt',\n",
       " 'neg/cv394_5311.txt',\n",
       " 'neg/cv395_11761.txt',\n",
       " 'neg/cv396_19127.txt',\n",
       " 'neg/cv397_28890.txt',\n",
       " 'neg/cv398_17047.txt',\n",
       " 'neg/cv399_28593.txt',\n",
       " 'neg/cv400_20631.txt',\n",
       " 'neg/cv401_13758.txt',\n",
       " 'neg/cv402_16097.txt',\n",
       " 'neg/cv403_6721.txt',\n",
       " 'neg/cv404_21805.txt',\n",
       " 'neg/cv405_21868.txt',\n",
       " 'neg/cv406_22199.txt',\n",
       " 'neg/cv407_23928.txt',\n",
       " 'neg/cv408_5367.txt',\n",
       " 'neg/cv409_29625.txt',\n",
       " 'neg/cv410_25624.txt',\n",
       " 'neg/cv411_16799.txt',\n",
       " 'neg/cv412_25254.txt',\n",
       " 'neg/cv413_7893.txt',\n",
       " 'neg/cv414_11161.txt',\n",
       " 'neg/cv415_23674.txt',\n",
       " 'neg/cv416_12048.txt',\n",
       " 'neg/cv417_14653.txt',\n",
       " 'neg/cv418_16562.txt',\n",
       " 'neg/cv419_14799.txt',\n",
       " 'neg/cv420_28631.txt',\n",
       " 'neg/cv421_9752.txt',\n",
       " 'neg/cv422_9632.txt',\n",
       " 'neg/cv423_12089.txt',\n",
       " 'neg/cv424_9268.txt',\n",
       " 'neg/cv425_8603.txt',\n",
       " 'neg/cv426_10976.txt',\n",
       " 'neg/cv427_11693.txt',\n",
       " 'neg/cv428_12202.txt',\n",
       " 'neg/cv429_7937.txt',\n",
       " 'neg/cv430_18662.txt',\n",
       " 'neg/cv431_7538.txt',\n",
       " 'neg/cv432_15873.txt',\n",
       " 'neg/cv433_10443.txt',\n",
       " 'neg/cv434_5641.txt',\n",
       " 'neg/cv435_24355.txt',\n",
       " 'neg/cv436_20564.txt',\n",
       " 'neg/cv437_24070.txt',\n",
       " 'neg/cv438_8500.txt',\n",
       " 'neg/cv439_17633.txt',\n",
       " 'neg/cv440_16891.txt',\n",
       " 'neg/cv441_15276.txt',\n",
       " 'neg/cv442_15499.txt',\n",
       " 'neg/cv443_22367.txt',\n",
       " 'neg/cv444_9975.txt',\n",
       " 'neg/cv445_26683.txt',\n",
       " 'neg/cv446_12209.txt',\n",
       " 'neg/cv447_27334.txt',\n",
       " 'neg/cv448_16409.txt',\n",
       " 'neg/cv449_9126.txt',\n",
       " 'neg/cv450_8319.txt',\n",
       " 'neg/cv451_11502.txt',\n",
       " 'neg/cv452_5179.txt',\n",
       " 'neg/cv453_10911.txt',\n",
       " 'neg/cv454_21961.txt',\n",
       " 'neg/cv455_28866.txt',\n",
       " 'neg/cv456_20370.txt',\n",
       " 'neg/cv457_19546.txt',\n",
       " 'neg/cv458_9000.txt',\n",
       " 'neg/cv459_21834.txt',\n",
       " 'neg/cv460_11723.txt',\n",
       " 'neg/cv461_21124.txt',\n",
       " 'neg/cv462_20788.txt',\n",
       " 'neg/cv463_10846.txt',\n",
       " 'neg/cv464_17076.txt',\n",
       " 'neg/cv465_23401.txt',\n",
       " 'neg/cv466_20092.txt',\n",
       " 'neg/cv467_26610.txt',\n",
       " 'neg/cv468_16844.txt',\n",
       " 'neg/cv469_21998.txt',\n",
       " 'neg/cv470_17444.txt',\n",
       " 'neg/cv471_18405.txt',\n",
       " 'neg/cv472_29140.txt',\n",
       " 'neg/cv473_7869.txt',\n",
       " 'neg/cv474_10682.txt',\n",
       " 'neg/cv475_22978.txt',\n",
       " 'neg/cv476_18402.txt',\n",
       " 'neg/cv477_23530.txt',\n",
       " 'neg/cv478_15921.txt',\n",
       " 'neg/cv479_5450.txt',\n",
       " 'neg/cv480_21195.txt',\n",
       " 'neg/cv481_7930.txt',\n",
       " 'neg/cv482_11233.txt',\n",
       " 'neg/cv483_18103.txt',\n",
       " 'neg/cv484_26169.txt',\n",
       " 'neg/cv485_26879.txt',\n",
       " 'neg/cv486_9788.txt',\n",
       " 'neg/cv487_11058.txt',\n",
       " 'neg/cv488_21453.txt',\n",
       " 'neg/cv489_19046.txt',\n",
       " 'neg/cv490_18986.txt',\n",
       " 'neg/cv491_12992.txt',\n",
       " 'neg/cv492_19370.txt',\n",
       " 'neg/cv493_14135.txt',\n",
       " 'neg/cv494_18689.txt',\n",
       " 'neg/cv495_16121.txt',\n",
       " 'neg/cv496_11185.txt',\n",
       " 'neg/cv497_27086.txt',\n",
       " 'neg/cv498_9288.txt',\n",
       " 'neg/cv499_11407.txt',\n",
       " 'neg/cv500_10722.txt',\n",
       " 'neg/cv501_12675.txt',\n",
       " 'neg/cv502_10970.txt',\n",
       " 'neg/cv503_11196.txt',\n",
       " 'neg/cv504_29120.txt',\n",
       " 'neg/cv505_12926.txt',\n",
       " 'neg/cv506_17521.txt',\n",
       " 'neg/cv507_9509.txt',\n",
       " 'neg/cv508_17742.txt',\n",
       " 'neg/cv509_17354.txt',\n",
       " 'neg/cv510_24758.txt',\n",
       " 'neg/cv511_10360.txt',\n",
       " 'neg/cv512_17618.txt',\n",
       " 'neg/cv513_7236.txt',\n",
       " 'neg/cv514_12173.txt',\n",
       " 'neg/cv515_18484.txt',\n",
       " 'neg/cv516_12117.txt',\n",
       " 'neg/cv517_20616.txt',\n",
       " 'neg/cv518_14798.txt',\n",
       " 'neg/cv519_16239.txt',\n",
       " 'neg/cv520_13297.txt',\n",
       " 'neg/cv521_1730.txt',\n",
       " 'neg/cv522_5418.txt',\n",
       " 'neg/cv523_18285.txt',\n",
       " 'neg/cv524_24885.txt',\n",
       " 'neg/cv525_17930.txt',\n",
       " 'neg/cv526_12868.txt',\n",
       " 'neg/cv527_10338.txt',\n",
       " 'neg/cv528_11669.txt',\n",
       " 'neg/cv529_10972.txt',\n",
       " 'neg/cv530_17949.txt',\n",
       " 'neg/cv531_26838.txt',\n",
       " 'neg/cv532_6495.txt',\n",
       " 'neg/cv533_9843.txt',\n",
       " 'neg/cv534_15683.txt',\n",
       " 'neg/cv535_21183.txt',\n",
       " 'neg/cv536_27221.txt',\n",
       " 'neg/cv537_13516.txt',\n",
       " 'neg/cv538_28485.txt',\n",
       " 'neg/cv539_21865.txt',\n",
       " 'neg/cv540_3092.txt',\n",
       " 'neg/cv541_28683.txt',\n",
       " 'neg/cv542_20359.txt',\n",
       " 'neg/cv543_5107.txt',\n",
       " 'neg/cv544_5301.txt',\n",
       " 'neg/cv545_12848.txt',\n",
       " 'neg/cv546_12723.txt',\n",
       " 'neg/cv547_18043.txt',\n",
       " 'neg/cv548_18944.txt',\n",
       " 'neg/cv549_22771.txt',\n",
       " 'neg/cv550_23226.txt',\n",
       " 'neg/cv551_11214.txt',\n",
       " 'neg/cv552_0150.txt',\n",
       " 'neg/cv553_26965.txt',\n",
       " 'neg/cv554_14678.txt',\n",
       " 'neg/cv555_25047.txt',\n",
       " 'neg/cv556_16563.txt',\n",
       " 'neg/cv557_12237.txt',\n",
       " 'neg/cv558_29376.txt',\n",
       " 'neg/cv559_0057.txt',\n",
       " 'neg/cv560_18608.txt',\n",
       " 'neg/cv561_9484.txt',\n",
       " 'neg/cv562_10847.txt',\n",
       " 'neg/cv563_18610.txt',\n",
       " 'neg/cv564_12011.txt',\n",
       " 'neg/cv565_29403.txt',\n",
       " 'neg/cv566_8967.txt',\n",
       " 'neg/cv567_29420.txt',\n",
       " 'neg/cv568_17065.txt',\n",
       " 'neg/cv569_26750.txt',\n",
       " 'neg/cv570_28960.txt',\n",
       " 'neg/cv571_29292.txt',\n",
       " 'neg/cv572_20053.txt',\n",
       " 'neg/cv573_29384.txt',\n",
       " 'neg/cv574_23191.txt',\n",
       " 'neg/cv575_22598.txt',\n",
       " 'neg/cv576_15688.txt',\n",
       " 'neg/cv577_28220.txt',\n",
       " 'neg/cv578_16825.txt',\n",
       " 'neg/cv579_12542.txt',\n",
       " 'neg/cv580_15681.txt',\n",
       " 'neg/cv581_20790.txt',\n",
       " 'neg/cv582_6678.txt',\n",
       " 'neg/cv583_29465.txt',\n",
       " 'neg/cv584_29549.txt',\n",
       " 'neg/cv585_23576.txt',\n",
       " 'neg/cv586_8048.txt',\n",
       " 'neg/cv587_20532.txt',\n",
       " 'neg/cv588_14467.txt',\n",
       " 'neg/cv589_12853.txt',\n",
       " 'neg/cv590_20712.txt',\n",
       " 'neg/cv591_24887.txt',\n",
       " 'neg/cv592_23391.txt',\n",
       " 'neg/cv593_11931.txt',\n",
       " 'neg/cv594_11945.txt',\n",
       " 'neg/cv595_26420.txt',\n",
       " 'neg/cv596_4367.txt',\n",
       " 'neg/cv597_26744.txt',\n",
       " 'neg/cv598_18184.txt',\n",
       " 'neg/cv599_22197.txt',\n",
       " 'neg/cv600_25043.txt',\n",
       " 'neg/cv601_24759.txt',\n",
       " 'neg/cv602_8830.txt',\n",
       " 'neg/cv603_18885.txt',\n",
       " 'neg/cv604_23339.txt',\n",
       " 'neg/cv605_12730.txt',\n",
       " 'neg/cv606_17672.txt',\n",
       " 'neg/cv607_8235.txt',\n",
       " 'neg/cv608_24647.txt',\n",
       " 'neg/cv609_25038.txt',\n",
       " 'neg/cv610_24153.txt',\n",
       " 'neg/cv611_2253.txt',\n",
       " 'neg/cv612_5396.txt',\n",
       " 'neg/cv613_23104.txt',\n",
       " 'neg/cv614_11320.txt',\n",
       " 'neg/cv615_15734.txt',\n",
       " 'neg/cv616_29187.txt',\n",
       " 'neg/cv617_9561.txt',\n",
       " 'neg/cv618_9469.txt',\n",
       " 'neg/cv619_13677.txt',\n",
       " 'neg/cv620_2556.txt',\n",
       " 'neg/cv621_15984.txt',\n",
       " 'neg/cv622_8583.txt',\n",
       " 'neg/cv623_16988.txt',\n",
       " 'neg/cv624_11601.txt',\n",
       " 'neg/cv625_13518.txt',\n",
       " 'neg/cv626_7907.txt',\n",
       " 'neg/cv627_12603.txt',\n",
       " 'neg/cv628_20758.txt',\n",
       " 'neg/cv629_16604.txt',\n",
       " 'neg/cv630_10152.txt',\n",
       " 'neg/cv631_4782.txt',\n",
       " 'neg/cv632_9704.txt',\n",
       " 'neg/cv633_29730.txt',\n",
       " 'neg/cv634_11989.txt',\n",
       " 'neg/cv635_0984.txt',\n",
       " 'neg/cv636_16954.txt',\n",
       " 'neg/cv637_13682.txt',\n",
       " 'neg/cv638_29394.txt',\n",
       " 'neg/cv639_10797.txt',\n",
       " 'neg/cv640_5380.txt',\n",
       " 'neg/cv641_13412.txt',\n",
       " 'neg/cv642_29788.txt',\n",
       " 'neg/cv643_29282.txt',\n",
       " 'neg/cv644_18551.txt',\n",
       " 'neg/cv645_17078.txt',\n",
       " 'neg/cv646_16817.txt',\n",
       " 'neg/cv647_15275.txt',\n",
       " 'neg/cv648_17277.txt',\n",
       " 'neg/cv649_13947.txt',\n",
       " 'neg/cv650_15974.txt',\n",
       " 'neg/cv651_11120.txt',\n",
       " 'neg/cv652_15653.txt',\n",
       " 'neg/cv653_2107.txt',\n",
       " 'neg/cv654_19345.txt',\n",
       " 'neg/cv655_12055.txt',\n",
       " 'neg/cv656_25395.txt',\n",
       " 'neg/cv657_25835.txt',\n",
       " 'neg/cv658_11186.txt',\n",
       " 'neg/cv659_21483.txt',\n",
       " 'neg/cv660_23140.txt',\n",
       " 'neg/cv661_25780.txt',\n",
       " 'neg/cv662_14791.txt',\n",
       " 'neg/cv663_14484.txt',\n",
       " 'neg/cv664_4264.txt',\n",
       " 'neg/cv665_29386.txt',\n",
       " 'neg/cv666_20301.txt',\n",
       " 'neg/cv667_19672.txt',\n",
       " 'neg/cv668_18848.txt',\n",
       " 'neg/cv669_24318.txt',\n",
       " 'neg/cv670_2666.txt',\n",
       " 'neg/cv671_5164.txt',\n",
       " 'neg/cv672_27988.txt',\n",
       " 'neg/cv673_25874.txt',\n",
       " 'neg/cv674_11593.txt',\n",
       " 'neg/cv675_22871.txt',\n",
       " 'neg/cv676_22202.txt',\n",
       " 'neg/cv677_18938.txt',\n",
       " 'neg/cv678_14887.txt',\n",
       " 'neg/cv679_28221.txt',\n",
       " 'neg/cv680_10533.txt',\n",
       " 'neg/cv681_9744.txt',\n",
       " 'neg/cv682_17947.txt',\n",
       " 'neg/cv683_13047.txt',\n",
       " 'neg/cv684_12727.txt',\n",
       " 'neg/cv685_5710.txt',\n",
       " 'neg/cv686_15553.txt',\n",
       " 'neg/cv687_22207.txt',\n",
       " 'neg/cv688_7884.txt',\n",
       " 'neg/cv689_13701.txt',\n",
       " 'neg/cv690_5425.txt',\n",
       " 'neg/cv691_5090.txt',\n",
       " 'neg/cv692_17026.txt',\n",
       " 'neg/cv693_19147.txt',\n",
       " 'neg/cv694_4526.txt',\n",
       " 'neg/cv695_22268.txt',\n",
       " 'neg/cv696_29619.txt',\n",
       " 'neg/cv697_12106.txt',\n",
       " 'neg/cv698_16930.txt',\n",
       " 'neg/cv699_7773.txt',\n",
       " 'neg/cv700_23163.txt',\n",
       " 'neg/cv701_15880.txt',\n",
       " 'neg/cv702_12371.txt',\n",
       " 'neg/cv703_17948.txt',\n",
       " 'neg/cv704_17622.txt',\n",
       " 'neg/cv705_11973.txt',\n",
       " 'neg/cv706_25883.txt',\n",
       " 'neg/cv707_11421.txt',\n",
       " 'neg/cv708_28539.txt',\n",
       " 'neg/cv709_11173.txt',\n",
       " 'neg/cv710_23745.txt',\n",
       " 'neg/cv711_12687.txt',\n",
       " 'neg/cv712_24217.txt',\n",
       " 'neg/cv713_29002.txt',\n",
       " 'neg/cv714_19704.txt',\n",
       " 'neg/cv715_19246.txt',\n",
       " 'neg/cv716_11153.txt',\n",
       " 'neg/cv717_17472.txt',\n",
       " 'neg/cv718_12227.txt',\n",
       " 'neg/cv719_5581.txt',\n",
       " 'neg/cv720_5383.txt',\n",
       " 'neg/cv721_28993.txt',\n",
       " 'neg/cv722_7571.txt',\n",
       " 'neg/cv723_9002.txt',\n",
       " 'neg/cv724_15265.txt',\n",
       " 'neg/cv725_10266.txt',\n",
       " 'neg/cv726_4365.txt',\n",
       " 'neg/cv727_5006.txt',\n",
       " 'neg/cv728_17931.txt',\n",
       " 'neg/cv729_10475.txt',\n",
       " 'neg/cv730_10729.txt',\n",
       " 'neg/cv731_3968.txt',\n",
       " 'neg/cv732_13092.txt',\n",
       " 'neg/cv733_9891.txt',\n",
       " 'neg/cv734_22821.txt',\n",
       " 'neg/cv735_20218.txt',\n",
       " 'neg/cv736_24947.txt',\n",
       " 'neg/cv737_28733.txt',\n",
       " 'neg/cv738_10287.txt',\n",
       " 'neg/cv739_12179.txt',\n",
       " 'neg/cv740_13643.txt',\n",
       " 'neg/cv741_12765.txt',\n",
       " 'neg/cv742_8279.txt',\n",
       " 'neg/cv743_17023.txt',\n",
       " 'neg/cv744_10091.txt',\n",
       " 'neg/cv745_14009.txt',\n",
       " 'neg/cv746_10471.txt',\n",
       " 'neg/cv747_18189.txt',\n",
       " 'neg/cv748_14044.txt',\n",
       " 'neg/cv749_18960.txt',\n",
       " 'neg/cv750_10606.txt',\n",
       " 'neg/cv751_17208.txt',\n",
       " 'neg/cv752_25330.txt',\n",
       " 'neg/cv753_11812.txt',\n",
       " 'neg/cv754_7709.txt',\n",
       " 'neg/cv755_24881.txt',\n",
       " 'neg/cv756_23676.txt',\n",
       " 'neg/cv757_10668.txt',\n",
       " 'neg/cv758_9740.txt',\n",
       " 'neg/cv759_15091.txt',\n",
       " 'neg/cv760_8977.txt',\n",
       " 'neg/cv761_13769.txt',\n",
       " 'neg/cv762_15604.txt',\n",
       " 'neg/cv763_16486.txt',\n",
       " 'neg/cv764_12701.txt',\n",
       " 'neg/cv765_20429.txt',\n",
       " 'neg/cv766_7983.txt',\n",
       " 'neg/cv767_15673.txt',\n",
       " 'neg/cv768_12709.txt',\n",
       " 'neg/cv769_8565.txt',\n",
       " 'neg/cv770_11061.txt',\n",
       " 'neg/cv771_28466.txt',\n",
       " 'neg/cv772_12971.txt',\n",
       " 'neg/cv773_20264.txt',\n",
       " 'neg/cv774_15488.txt',\n",
       " 'neg/cv775_17966.txt',\n",
       " 'neg/cv776_21934.txt',\n",
       " 'neg/cv777_10247.txt',\n",
       " 'neg/cv778_18629.txt',\n",
       " 'neg/cv779_18989.txt',\n",
       " 'neg/cv780_8467.txt',\n",
       " 'neg/cv781_5358.txt',\n",
       " 'neg/cv782_21078.txt',\n",
       " 'neg/cv783_14724.txt',\n",
       " 'neg/cv784_16077.txt',\n",
       " 'neg/cv785_23748.txt',\n",
       " 'neg/cv786_23608.txt',\n",
       " 'neg/cv787_15277.txt',\n",
       " 'neg/cv788_26409.txt',\n",
       " 'neg/cv789_12991.txt',\n",
       " 'neg/cv790_16202.txt',\n",
       " 'neg/cv791_17995.txt',\n",
       " 'neg/cv792_3257.txt',\n",
       " 'neg/cv793_15235.txt',\n",
       " 'neg/cv794_17353.txt',\n",
       " 'neg/cv795_10291.txt',\n",
       " 'neg/cv796_17243.txt',\n",
       " 'neg/cv797_7245.txt',\n",
       " 'neg/cv798_24779.txt',\n",
       " 'neg/cv799_19812.txt',\n",
       " 'neg/cv800_13494.txt',\n",
       " 'neg/cv801_26335.txt',\n",
       " 'neg/cv802_28381.txt',\n",
       " 'neg/cv803_8584.txt',\n",
       " 'neg/cv804_11763.txt',\n",
       " 'neg/cv805_21128.txt',\n",
       " 'neg/cv806_9405.txt',\n",
       " 'neg/cv807_23024.txt',\n",
       " 'neg/cv808_13773.txt',\n",
       " 'neg/cv809_5012.txt',\n",
       " 'neg/cv810_13660.txt',\n",
       " 'neg/cv811_22646.txt',\n",
       " 'neg/cv812_19051.txt',\n",
       " 'neg/cv813_6649.txt',\n",
       " 'neg/cv814_20316.txt',\n",
       " 'neg/cv815_23466.txt',\n",
       " 'neg/cv816_15257.txt',\n",
       " 'neg/cv817_3675.txt',\n",
       " 'neg/cv818_10698.txt',\n",
       " 'neg/cv819_9567.txt',\n",
       " 'neg/cv820_24157.txt',\n",
       " 'neg/cv821_29283.txt',\n",
       " 'neg/cv822_21545.txt',\n",
       " 'neg/cv823_17055.txt',\n",
       " 'neg/cv824_9335.txt',\n",
       " 'neg/cv825_5168.txt',\n",
       " 'neg/cv826_12761.txt',\n",
       " 'neg/cv827_19479.txt',\n",
       " 'neg/cv828_21392.txt',\n",
       " 'neg/cv829_21725.txt',\n",
       " 'neg/cv830_5778.txt',\n",
       " 'neg/cv831_16325.txt',\n",
       " 'neg/cv832_24713.txt',\n",
       " 'neg/cv833_11961.txt',\n",
       " 'neg/cv834_23192.txt',\n",
       " 'neg/cv835_20531.txt',\n",
       " 'neg/cv836_14311.txt',\n",
       " 'neg/cv837_27232.txt',\n",
       " 'neg/cv838_25886.txt',\n",
       " 'neg/cv839_22807.txt',\n",
       " 'neg/cv840_18033.txt',\n",
       " 'neg/cv841_3367.txt',\n",
       " 'neg/cv842_5702.txt',\n",
       " 'neg/cv843_17054.txt',\n",
       " 'neg/cv844_13890.txt',\n",
       " 'neg/cv845_15886.txt',\n",
       " 'neg/cv846_29359.txt',\n",
       " 'neg/cv847_20855.txt',\n",
       " 'neg/cv848_10061.txt',\n",
       " 'neg/cv849_17215.txt',\n",
       " 'neg/cv850_18185.txt',\n",
       " 'neg/cv851_21895.txt',\n",
       " 'neg/cv852_27512.txt',\n",
       " 'neg/cv853_29119.txt',\n",
       " 'neg/cv854_18955.txt',\n",
       " 'neg/cv855_22134.txt',\n",
       " 'neg/cv856_28882.txt',\n",
       " 'neg/cv857_17527.txt',\n",
       " 'neg/cv858_20266.txt',\n",
       " 'neg/cv859_15689.txt',\n",
       " 'neg/cv860_15520.txt',\n",
       " 'neg/cv861_12809.txt',\n",
       " 'neg/cv862_15924.txt',\n",
       " 'neg/cv863_7912.txt',\n",
       " 'neg/cv864_3087.txt',\n",
       " 'neg/cv865_28796.txt',\n",
       " 'neg/cv866_29447.txt',\n",
       " 'neg/cv867_18362.txt',\n",
       " 'neg/cv868_12799.txt',\n",
       " 'neg/cv869_24782.txt',\n",
       " 'neg/cv870_18090.txt',\n",
       " 'neg/cv871_25971.txt',\n",
       " 'neg/cv872_13710.txt',\n",
       " 'neg/cv873_19937.txt',\n",
       " 'neg/cv874_12182.txt',\n",
       " 'neg/cv875_5622.txt',\n",
       " 'neg/cv876_9633.txt',\n",
       " 'neg/cv877_29132.txt',\n",
       " 'neg/cv878_17204.txt',\n",
       " 'neg/cv879_16585.txt',\n",
       " 'neg/cv880_29629.txt',\n",
       " 'neg/cv881_14767.txt',\n",
       " 'neg/cv882_10042.txt',\n",
       " 'neg/cv883_27621.txt',\n",
       " 'neg/cv884_15230.txt',\n",
       " 'neg/cv885_13390.txt',\n",
       " 'neg/cv886_19210.txt',\n",
       " 'neg/cv887_5306.txt',\n",
       " 'neg/cv888_25678.txt',\n",
       " 'neg/cv889_22670.txt',\n",
       " 'neg/cv890_3515.txt',\n",
       " 'neg/cv891_6035.txt',\n",
       " 'neg/cv892_18788.txt',\n",
       " 'neg/cv893_26731.txt',\n",
       " 'neg/cv894_22140.txt',\n",
       " 'neg/cv895_22200.txt',\n",
       " 'neg/cv896_17819.txt',\n",
       " 'neg/cv897_11703.txt',\n",
       " 'neg/cv898_1576.txt',\n",
       " 'neg/cv899_17812.txt',\n",
       " 'neg/cv900_10800.txt',\n",
       " 'neg/cv901_11934.txt',\n",
       " 'neg/cv902_13217.txt',\n",
       " 'neg/cv903_18981.txt',\n",
       " 'neg/cv904_25663.txt',\n",
       " 'neg/cv905_28965.txt',\n",
       " 'neg/cv906_12332.txt',\n",
       " 'neg/cv907_3193.txt',\n",
       " 'neg/cv908_17779.txt',\n",
       " 'neg/cv909_9973.txt',\n",
       " 'neg/cv910_21930.txt',\n",
       " 'neg/cv911_21695.txt',\n",
       " 'neg/cv912_5562.txt',\n",
       " 'neg/cv913_29127.txt',\n",
       " 'neg/cv914_2856.txt',\n",
       " 'neg/cv915_9342.txt',\n",
       " 'neg/cv916_17034.txt',\n",
       " 'neg/cv917_29484.txt',\n",
       " 'neg/cv918_27080.txt',\n",
       " 'neg/cv919_18155.txt',\n",
       " 'neg/cv920_29423.txt',\n",
       " 'neg/cv921_13988.txt',\n",
       " 'neg/cv922_10185.txt',\n",
       " 'neg/cv923_11951.txt',\n",
       " 'neg/cv924_29397.txt',\n",
       " 'neg/cv925_9459.txt',\n",
       " 'neg/cv926_18471.txt',\n",
       " 'neg/cv927_11471.txt',\n",
       " 'neg/cv928_9478.txt',\n",
       " 'neg/cv929_1841.txt',\n",
       " 'neg/cv930_14949.txt',\n",
       " 'neg/cv931_18783.txt',\n",
       " 'neg/cv932_14854.txt',\n",
       " 'neg/cv933_24953.txt',\n",
       " 'neg/cv934_20426.txt',\n",
       " 'neg/cv935_24977.txt',\n",
       " 'neg/cv936_17473.txt',\n",
       " 'neg/cv937_9816.txt',\n",
       " 'neg/cv938_10706.txt',\n",
       " 'neg/cv939_11247.txt',\n",
       " 'neg/cv940_18935.txt',\n",
       " 'neg/cv941_10718.txt',\n",
       " 'neg/cv942_18509.txt',\n",
       " 'neg/cv943_23547.txt',\n",
       " 'neg/cv944_15042.txt',\n",
       " 'neg/cv945_13012.txt',\n",
       " 'neg/cv946_20084.txt',\n",
       " 'neg/cv947_11316.txt',\n",
       " 'neg/cv948_25870.txt',\n",
       " 'neg/cv949_21565.txt',\n",
       " 'neg/cv950_13478.txt',\n",
       " 'neg/cv951_11816.txt',\n",
       " 'neg/cv952_26375.txt',\n",
       " 'neg/cv953_7078.txt',\n",
       " 'neg/cv954_19932.txt',\n",
       " 'neg/cv955_26154.txt',\n",
       " 'neg/cv956_12547.txt',\n",
       " 'neg/cv957_9059.txt',\n",
       " 'neg/cv958_13020.txt',\n",
       " 'neg/cv959_16218.txt',\n",
       " 'neg/cv960_28877.txt',\n",
       " 'neg/cv961_5578.txt',\n",
       " 'neg/cv962_9813.txt',\n",
       " 'neg/cv963_7208.txt',\n",
       " 'neg/cv964_5794.txt',\n",
       " 'neg/cv965_26688.txt',\n",
       " 'neg/cv966_28671.txt',\n",
       " 'neg/cv967_5626.txt',\n",
       " 'neg/cv968_25413.txt',\n",
       " 'neg/cv969_14760.txt',\n",
       " 'neg/cv970_19532.txt',\n",
       " 'neg/cv971_11790.txt',\n",
       " 'neg/cv972_26837.txt',\n",
       " 'neg/cv973_10171.txt',\n",
       " 'neg/cv974_24303.txt',\n",
       " 'neg/cv975_11920.txt',\n",
       " 'neg/cv976_10724.txt',\n",
       " 'neg/cv977_4776.txt',\n",
       " 'neg/cv978_22192.txt',\n",
       " 'neg/cv979_2029.txt',\n",
       " 'neg/cv980_11851.txt',\n",
       " 'neg/cv981_16679.txt',\n",
       " 'neg/cv982_22209.txt',\n",
       " 'neg/cv983_24219.txt',\n",
       " 'neg/cv984_14006.txt',\n",
       " 'neg/cv985_5964.txt',\n",
       " 'neg/cv986_15092.txt',\n",
       " 'neg/cv987_7394.txt',\n",
       " 'neg/cv988_20168.txt',\n",
       " 'neg/cv989_17297.txt',\n",
       " 'neg/cv990_12443.txt',\n",
       " 'neg/cv991_19973.txt',\n",
       " 'neg/cv992_12806.txt',\n",
       " 'neg/cv993_29565.txt',\n",
       " 'neg/cv994_13229.txt',\n",
       " 'neg/cv995_23113.txt',\n",
       " 'neg/cv996_12447.txt',\n",
       " 'neg/cv997_5152.txt',\n",
       " 'neg/cv998_15691.txt',\n",
       " 'neg/cv999_14636.txt']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if', 'you', \"'\", 've', 'ever', 'perused', 'my', ...]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's take a positive review \n",
    "rev = nltk.corpus.movie_reviews.words('pos/cv664_4389.txt')\n",
    "rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as you see, the file is already tokenized\n",
    "### it is generally used for us to do tokenization\n",
    "### inorder to user countvectorizer, we have to pass strings instead of tokens\n",
    "### for that we have to convert all those tokens into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in neg_rev:\n",
    "    rev_text_neg = rev = nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string = \" \".join(rev_text_neg)\n",
    "    review_one_string = review_one_string.replace(',', ',')\n",
    "    review_one_string = review_one_string.replace('.', '.')\n",
    "    review_one_string = review_one_string.replace(\"\\'\", \"'\")\n",
    "    review_one_string = review_one_string.replace(\" \\'\", \"'\")\n",
    "    \n",
    "    rev_list.append(review_one_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rev_list)\n",
    "#negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_pos = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos_rev in rev_pos:\n",
    "    rev_text_pos = rev = nltk.corpus.movie_reviews.words(pos_rev)\n",
    "    review_one_string = \" \".join(rev_text_pos)\n",
    "    review_one_string = review_one_string.replace(',', ',')\n",
    "    review_one_string = review_one_string.replace('.', '.')\n",
    "    review_one_string = review_one_string.replace(\"\\'\", \"'\")\n",
    "    review_one_string = review_one_string.replace(\" \\'\", \"'\")\n",
    "    \n",
    "    rev_list.append(review_one_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rev_list)\n",
    "#positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create targets before creating features\n",
    "neg_targets = np.zeros((1000,),dtype=np.int)#neg reviews as 0\n",
    "pos_targets = np.ones((1000,), dtype=np.int)#pos reviews as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for neg_tar in neg_targets:\n",
    "    target_list.append(neg_tar)\n",
    "for pos_tar in pos_targets:\n",
    "    target_list.append(pos_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create pandas series for target list\n",
    "y = pd.Series(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's have a look on first 5 in this series\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(lowercase=True, stop_words=\"english\", min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after initialising the vectorizer, we need to fit into rev_list\n",
    "X_count_vect = count_vect.fit_transform(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 23784)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimensions of this vector\n",
    "X_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '007',\n",
       " '05',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '100m',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '113',\n",
       " '115',\n",
       " '11th',\n",
       " '12',\n",
       " '126',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '132',\n",
       " '137',\n",
       " '13th',\n",
       " '14',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '1500s',\n",
       " '155',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '161',\n",
       " '16mm',\n",
       " '16th',\n",
       " '16x9',\n",
       " '17',\n",
       " '175',\n",
       " '1773',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1800s',\n",
       " '1839',\n",
       " '1869',\n",
       " '1871',\n",
       " '1888',\n",
       " '18th',\n",
       " '19',\n",
       " '1900',\n",
       " '1912',\n",
       " '1914',\n",
       " '1919',\n",
       " '1925',\n",
       " '1928',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1932',\n",
       " '1933',\n",
       " '1935',\n",
       " '1937',\n",
       " '1938',\n",
       " '1939',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1941',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1953',\n",
       " '1954',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1998s',\n",
       " '1999',\n",
       " '19th',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2013',\n",
       " '2017',\n",
       " '2020',\n",
       " '2029',\n",
       " '2050',\n",
       " '2058',\n",
       " '20s',\n",
       " '20th',\n",
       " '21',\n",
       " '2176',\n",
       " '21st',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '2400',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '25th',\n",
       " '26',\n",
       " '26th',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '2d',\n",
       " '2nd',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30th',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '35',\n",
       " '35mm',\n",
       " '36',\n",
       " '360',\n",
       " '37',\n",
       " '39',\n",
       " '3d',\n",
       " '3po',\n",
       " '3rd',\n",
       " '40',\n",
       " '400',\n",
       " '40s',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '47',\n",
       " '48',\n",
       " '48th',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50s',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '571',\n",
       " '58',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '6000',\n",
       " '607',\n",
       " '60s',\n",
       " '61',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '666',\n",
       " '69',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '70mm',\n",
       " '70s',\n",
       " '73',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '80s',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '8mm',\n",
       " '8th',\n",
       " '90',\n",
       " '90210',\n",
       " '90s',\n",
       " '91',\n",
       " '911',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '999',\n",
       " '9mm',\n",
       " '_____',\n",
       " '______',\n",
       " '_and_',\n",
       " '_babe_',\n",
       " '_blade',\n",
       " '_brazil_',\n",
       " '_do_',\n",
       " '_does_',\n",
       " '_don',\n",
       " '_film',\n",
       " '_in',\n",
       " '_is_',\n",
       " '_last_',\n",
       " '_life',\n",
       " '_must_',\n",
       " '_not_',\n",
       " '_real_',\n",
       " '_really_',\n",
       " '_saturday_night_live_',\n",
       " '_saving',\n",
       " '_scream_',\n",
       " '_that_',\n",
       " '_the',\n",
       " '_the_',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaliyah',\n",
       " 'aardman',\n",
       " 'aaron',\n",
       " 'aback',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandons',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abdomen',\n",
       " 'abducted',\n",
       " 'abduction',\n",
       " 'abductions',\n",
       " 'abe',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'aberration',\n",
       " 'abetted',\n",
       " 'abetting',\n",
       " 'abhorrent',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ably',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abolish',\n",
       " 'abolitionists',\n",
       " 'abominable',\n",
       " 'abomination',\n",
       " 'aborted',\n",
       " 'abortion',\n",
       " 'abortions',\n",
       " 'abound',\n",
       " 'abounding',\n",
       " 'abounds',\n",
       " 'abraham',\n",
       " 'abrahams',\n",
       " 'abrams',\n",
       " 'abrasive',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absinthe',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolution',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'absorption',\n",
       " 'abstinence',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absurdist',\n",
       " 'absurdities',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusers',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abuzz',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accents',\n",
       " 'accentuate',\n",
       " 'accentuates',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'accolades',\n",
       " 'accommodates',\n",
       " 'accommodating',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompaniment',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplice',\n",
       " 'accomplices',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accordion',\n",
       " 'accosted',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accounted',\n",
       " 'accounts',\n",
       " 'accumulated',\n",
       " 'accumulation',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'ace',\n",
       " 'acerbic',\n",
       " 'aces',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achiever',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achilles',\n",
       " 'achingly',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'ack',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'acme',\n",
       " 'acquaintance',\n",
       " 'acquaintances',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquit',\n",
       " 'acquits',\n",
       " 'acquittal',\n",
       " 'acquitted',\n",
       " 'acres',\n",
       " 'acrimonious',\n",
       " 'acrobatic',\n",
       " 'acrobatics',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actioner',\n",
       " 'actioners',\n",
       " 'actionfest',\n",
       " 'actions',\n",
       " 'activated',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actualization',\n",
       " 'actualizing',\n",
       " 'actually',\n",
       " 'acumen',\n",
       " 'acupuncture',\n",
       " 'acute',\n",
       " 'acutely',\n",
       " 'ad',\n",
       " 'ad2am',\n",
       " 'adage',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'adaption',\n",
       " 'add',\n",
       " 'addams',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'addled',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'addy',\n",
       " 'ade',\n",
       " 'adefarasin',\n",
       " 'adept',\n",
       " 'adeptly',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhere',\n",
       " 'adherence',\n",
       " 'adheres',\n",
       " 'adjacent',\n",
       " 'adjani',\n",
       " 'adjective',\n",
       " 'adjectives',\n",
       " 'adjoining',\n",
       " 'adjust',\n",
       " 'adjuster',\n",
       " 'adjusting',\n",
       " 'administration',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admirer',\n",
       " 'admirers',\n",
       " 'admires',\n",
       " 'admiring',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admittance',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'admittingly',\n",
       " 'admonition',\n",
       " 'ado',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adolescents',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adoptive',\n",
       " 'adopts',\n",
       " 'adorable',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adored',\n",
       " 'adores',\n",
       " 'adorned',\n",
       " 'adrenalin',\n",
       " 'adrenaline',\n",
       " 'adrian',\n",
       " 'adrien',\n",
       " 'adrift',\n",
       " 'adroitly',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adulterous',\n",
       " 'adultery',\n",
       " 'adulthood',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancement',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'adventurer',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'adversarial',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversity',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertisements',\n",
       " 'advertiser',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advisable',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'adviser',\n",
       " 'advisers',\n",
       " 'advises',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advocate',\n",
       " 'advocating',\n",
       " 'aerial',\n",
       " 'aerosmith',\n",
       " 'aesthetic',\n",
       " 'aesthetically',\n",
       " 'aesthetics',\n",
       " 'afar',\n",
       " 'affability',\n",
       " 'affable',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affectations',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affectionately',\n",
       " 'affections',\n",
       " 'affects',\n",
       " 'affiliate',\n",
       " 'affiliated',\n",
       " 'affinity',\n",
       " 'affirmation',\n",
       " 'affirmative',\n",
       " 'affirming',\n",
       " 'affleck',\n",
       " 'afflicted',\n",
       " 'affliction',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afforded',\n",
       " 'affraid',\n",
       " 'affront',\n",
       " 'afi',\n",
       " 'aficionado',\n",
       " 'aficionados',\n",
       " 'afield',\n",
       " 'afloat',\n",
       " 'afo',\n",
       " 'afoot',\n",
       " 'afore',\n",
       " 'aforementioned',\n",
       " 'aformentioned',\n",
       " 'afoul',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africans',\n",
       " 'afro',\n",
       " 'aftereffects',\n",
       " 'afterglow',\n",
       " 'afterlife',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'aftertaste',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'agape',\n",
       " 'agatha',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'ageing',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggravating',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'agile',\n",
       " 'agility',\n",
       " 'aging',\n",
       " 'agitated',\n",
       " 'ago',\n",
       " 'agonizing',\n",
       " 'agonizingly',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agreeable',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'ah',\n",
       " 'ahabs',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahern',\n",
       " 'ahh',\n",
       " 'ahmed',\n",
       " 'aid',\n",
       " 'aidan',\n",
       " 'aide',\n",
       " 'aided',\n",
       " 'aiding',\n",
       " 'aids',\n",
       " 'aiello',\n",
       " 'ailing',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aimee',\n",
       " 'aiming',\n",
       " 'aimless',\n",
       " 'aimlessly',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'aired',\n",
       " 'aires',\n",
       " 'airline',\n",
       " 'airliner',\n",
       " 'airlock',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airwaves',\n",
       " 'airwolf',\n",
       " 'airy',\n",
       " 'aisle',\n",
       " 'aisles',\n",
       " 'aka',\n",
       " 'aki',\n",
       " 'akin',\n",
       " 'akira',\n",
       " 'akiva',\n",
       " 'akroyd',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'aladdin',\n",
       " 'alain',\n",
       " 'alan',\n",
       " 'alanis',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alas',\n",
       " 'alaska',\n",
       " 'albania',\n",
       " 'albanian',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'albertson',\n",
       " 'albino',\n",
       " 'albinos',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'alcoholics',\n",
       " 'alcoholism',\n",
       " 'alcott',\n",
       " 'alda',\n",
       " 'aldys',\n",
       " 'alec',\n",
       " 'alejandro',\n",
       " 'alek',\n",
       " 'alert',\n",
       " 'alerted',\n",
       " 'alessandro',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alexandra',\n",
       " 'alexandre',\n",
       " 'alfonso',\n",
       " 'alfre',\n",
       " 'alfred',\n",
       " 'algar',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'aliases',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alida',\n",
       " 'alien',\n",
       " 'alienate',\n",
       " 'alienated',\n",
       " 'alienates',\n",
       " 'alienating',\n",
       " 'alienation',\n",
       " 'aliens',\n",
       " 'align',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alison',\n",
       " 'alive',\n",
       " 'allah',\n",
       " 'allan',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'allegiances',\n",
       " 'allegory',\n",
       " 'allegra',\n",
       " 'allen',\n",
       " 'allergy',\n",
       " 'alleviate',\n",
       " 'alley',\n",
       " 'alleys',\n",
       " 'alliance',\n",
       " 'alliances',\n",
       " 'allied',\n",
       " 'allies',\n",
       " 'alligator',\n",
       " 'alligators',\n",
       " 'allison',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alluded',\n",
       " 'allure',\n",
       " 'alluring',\n",
       " 'allusion',\n",
       " 'allusions',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almod',\n",
       " 'alongside',\n",
       " 'aloof',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alright',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'alteration',\n",
       " 'alterations',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternately',\n",
       " 'alternates',\n",
       " 'alternating',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alters',\n",
       " 'althea',\n",
       " 'altman',\n",
       " 'altogether',\n",
       " 'altough',\n",
       " 'alum',\n",
       " 'alumni',\n",
       " 'alumnus',\n",
       " 'alvarado',\n",
       " 'alvin',\n",
       " 'alyson',\n",
       " 'alyssa',\n",
       " 'alzheimer',\n",
       " 'amadeus',\n",
       " 'amalgamation',\n",
       " 'amanda',\n",
       " 'amarillo',\n",
       " 'amateur',\n",
       " 'amateurish',\n",
       " 'amateurishly',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazement',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'ambassadors',\n",
       " 'amber',\n",
       " 'ambiance',\n",
       " 'ambience',\n",
       " 'ambient',\n",
       " 'ambiguities',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambiguously',\n",
       " 'ambition',\n",
       " 'ambitions',\n",
       " 'ambitious',\n",
       " 'ambitiously',\n",
       " 'ambivalence',\n",
       " 'ambivalent',\n",
       " 'ambrose',\n",
       " 'ambulance',\n",
       " 'ambush',\n",
       " 'amc',\n",
       " 'amen',\n",
       " 'amendment',\n",
       " 'amends',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanized',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'ames',\n",
       " 'amiable',\n",
       " 'amicable',\n",
       " 'amid',\n",
       " 'amidala',\n",
       " 'amidst',\n",
       " 'amis',\n",
       " 'amish',\n",
       " 'amiss',\n",
       " 'amistad',\n",
       " 'ammo',\n",
       " 'ammunition',\n",
       " 'amnesia',\n",
       " 'amnesiac',\n",
       " 'amok',\n",
       " 'amon',\n",
       " 'amoral',\n",
       " 'amorality',\n",
       " 'amos',\n",
       " 'amounted',\n",
       " 'amounts',\n",
       " 'amphetamines',\n",
       " 'amphibian',\n",
       " 'ample',\n",
       " 'amplified',\n",
       " 'amply',\n",
       " 'amputated',\n",
       " 'amuck',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusement',\n",
       " 'amusements',\n",
       " 'amusing',\n",
       " 'amusingly',\n",
       " 'amy',\n",
       " 'anaconda',\n",
       " 'anacondas',\n",
       " 'anakin',\n",
       " 'anal',\n",
       " 'analogy',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analysts',\n",
       " 'analyze',\n",
       " 'analyzed',\n",
       " 'analyzing',\n",
       " 'anand',\n",
       " 'anarchic',\n",
       " 'anarchists',\n",
       " 'anarchy',\n",
       " 'anastasia',\n",
       " 'anatomy',\n",
       " 'ancestors',\n",
       " 'anchor',\n",
       " 'anchors',\n",
       " 'ancient',\n",
       " 'anders',\n",
       " 'anderson',\n",
       " 'andersons',\n",
       " 'andie',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andreas',\n",
       " 'andrew',\n",
       " 'andrews',\n",
       " 'android',\n",
       " 'androids',\n",
       " 'andromeda',\n",
       " 'andrzej',\n",
       " 'andy',\n",
       " 'anecdotal',\n",
       " 'anecdote',\n",
       " 'anecdotes',\n",
       " 'anemic',\n",
       " 'anew',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelic',\n",
       " 'angelina',\n",
       " 'angelo',\n",
       " 'angels',\n",
       " 'anger',\n",
       " ...]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create list all the features\n",
    "X_names = count_vect.get_feature_names()\n",
    "X_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 23784)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's create a Dataframe by passing feature names as column names \n",
    "X_count_vect = pd.DataFrame(X_count_vect.toarray(), columns=X_names)\n",
    "X_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100m</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zoot</th>\n",
       "      <th>zorg</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuko</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwigoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  007  05  10  100  1000  100m  101  102   ...     zoom  zooming  \\\n",
       "0   0    0    0   0  10    0     0     0    0    0   ...        0        0   \n",
       "1   0    0    0   0   0    0     0     0    0    0   ...        0        0   \n",
       "2   0    0    0   0   0    0     0     0    0    0   ...        0        0   \n",
       "3   0    0    0   0   0    0     0     0    0    0   ...        0        0   \n",
       "4   0    0    0   0   0    0     0     0    0    0   ...        0        0   \n",
       "\n",
       "   zooms  zoot  zorg  zorro  zucker  zuko  zwick  zwigoff  \n",
       "0      0     0     0      0       0     0      0        0  \n",
       "1      0     0     0      0       0     0      0        0  \n",
       "2      0     0     0      0       0     0      0        0  \n",
       "3      0     0     0      0       0     0      0        0  \n",
       "4      0     0     0      0       0     0      0        0  \n",
       "\n",
       "[5 rows x 23784 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 5 rows of DF\n",
    "X_count_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's split our dataframe into train and test \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_count_vect, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 23784)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 23784)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so for classifying we are using Naive bayes classifier technic\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, Y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n"
     ]
    }
   ],
   "source": [
    "#let's check the accuracy of this metrics\n",
    "print(metrics.accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[213,  45],\n",
       "       [ 56, 186]], dtype=int64)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check with confusion matrix\n",
    "score_clf = confusion_matrix(Y_test, y_pred)\n",
    "score_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
